{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83d\udd22  CausalIQ Data","text":""},{"location":"#welcome","title":"Welcome","text":"<p>Welcome to the documentation for CausalIQ Data \u2014 part of the CausalIQ ecosystem for intelligent causal discovery. </p> <p>The CausalIQ Data project provides the data-related capabilities that causal discovery requires. </p>"},{"location":"#overview","title":"Overview","text":"<p>CausalIQ Data provides:</p> <ul> <li>\u26a1 data import and caching - data can be imported from standard  tabular formats (comma-separated variables) and cached for high performance</li> <li>\ud83c\udfaf graph scoring - provide graph score derived from the data which is  the objective function used by score-based structure learning algorithms. This is   based upon how likely the data is to be seen for a given graph, typically  modified by a penalty for complex graphs (e.g. BIC score), or modified  by a prior belief about the graph strcuture (e.g. BDeu score)</li> <li>\ud83d\udd17 independence tests - used to determine conditional independence tests  which are intrinsic to the operataion of constraint-based structure  learning algorithms.</li> </ul> <p>This site provides detailed documentation, including: development roadmap, user guide, architectural vision, design notes, and API reference for users and contributors.</p>"},{"location":"#quickstart-installation","title":"Quickstart &amp; Installation","text":"<p>For a quickstart guide and installation instructions, see the README on GitHub.</p>"},{"location":"#documentation-contents","title":"Documentation Contents","text":"<ul> <li>Development Roadmap: roadmap of upcoming features</li> <li>User Guide: comprehensive user guide</li> <li>Architecture: overall architecture and design notes</li> <li>API Reference: complete reference for Python code</li> <li>Development Guidelines: CausalIQ guidelines for developers</li> <li>Changelog</li> <li>License</li> </ul>"},{"location":"#support-community","title":"Support &amp; Community","text":"<ul> <li>GitHub Issues: Report bugs or request features.</li> <li>GitHub Discussions: Ask questions and join the community.</li> </ul> <p>Tip: Use the navigation sidebar to explore the documentation. For the latest code and releases, visit the causaliq-data GitHub repository.</p> <p>Supported Python Versions: 3.9, 3.10, 3.11, 3.12 Default Python Version: 3.11</p>"},{"location":"roadmap/","title":"CausalIQ Data - Development Roadmap","text":"<p>Last updated: December 19, 2025  </p> <p>This project roadmap fits into the overall ecosystem roadmap</p>"},{"location":"roadmap/#under-development","title":"\ud83d\udea7  Under development","text":"<ul> <li>none</li> </ul>"},{"location":"roadmap/#implemented-features","title":"\u2705 Implemented Features","text":"<ul> <li>Release v0.1.0 - Foundation Data [December 2025]: CausalIQ compliant Data provider interface and concrete implementations with data store internally as pandas Dataframes or Numpy 2D arrays.</li> <li>Release v0.2.0 - Score [December 2025]: Support for BIC and BDeu score functions</li> </ul> <p>See Git commit history for detailed implementation progress</p>"},{"location":"roadmap/#upcoming-releases","title":"\ud83d\udee3\ufe0f Upcoming Releases","text":"<ul> <li>Release v0.3.0 - CI Tests [December 2025]: Conditional Independence</li> </ul>"},{"location":"api/cli/","title":"Data CLI","text":"<p>This template repo provides a simple template CLI as a starting point for CLI implementations.</p>"},{"location":"api/cli/#cli-entry-point","title":"CLI entry point","text":"<p>This is the entry point for the CLI logic.</p>"},{"location":"api/cli/#causaliq_data.cli","title":"cli","text":"<p>Command-line interface for causaliq-data.</p> <p>Functions:</p> <ul> <li> <code>cli</code>             \u2013              <p>Simple CLI example.</p> </li> <li> <code>main</code>             \u2013              <p>Entry point for the CLI.</p> </li> </ul>"},{"location":"api/cli/#causaliq_data.cli.cli","title":"cli","text":"<pre><code>cli(name: str, greet: str) -&gt; None\n</code></pre> <p>Simple CLI example.</p> <p>NAME is the person to greet</p>"},{"location":"api/cli/#causaliq_data.cli.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>Entry point for the CLI.</p>"},{"location":"api/data/","title":"Data - Abstract Base Class","text":"<p>The <code>Data</code> class serves as the abstract base class for all data adapters in CausalIQ Data. It extends the BNFit interface from causaliq-core and provides the foundation for the plug-in data adapter architecture.</p>"},{"location":"api/data/#class-definition","title":"Class Definition","text":"<pre><code>class Data(BNFit):\n    \"\"\"Top level data object that implements BNFit interface.\n\n    Extends BNFit interface with additional methods needed for\n    causal structure learning algorithms.\n    \"\"\"\n</code></pre>"},{"location":"api/data/#key-attributes","title":"Key Attributes","text":"<ul> <li><code>elapsed</code>: Elapsed time for operations</li> <li><code>order</code>: Order in which nodes should be processed</li> <li><code>ext_to_orig</code>: Mapping from external to original node names</li> <li><code>orig_to_ext</code>: Mapping from original to external node names  </li> <li><code>dstype</code>: Overall dataset type (categorical/continuous/mixed)</li> </ul>"},{"location":"api/data/#core-methods","title":"Core Methods","text":""},{"location":"api/data/#node-order-management","title":"Node Order Management","text":""},{"location":"api/data/#set_orderorder-tuplestr-none","title":"<code>set_order(order: Tuple[str, ...]) -&gt; None</code>","text":"<p>Sets the processing order of nodes to the specified sequence.</p> <p>Arguments: - <code>order</code>: New processing order as a tuple of external node names</p> <p>Raises: - <code>TypeError</code>: For invalid argument types - <code>ValueError</code>: If order contains invalid node names</p>"},{"location":"api/data/#get_order-tuplestr","title":"<code>get_order() -&gt; Tuple[str, ...]</code>","text":"<p>Returns the current processing order using external node names.</p>"},{"location":"api/data/#randomise_orderseed-int-none","title":"<code>randomise_order(seed: int) -&gt; None</code>","text":"<p>Randomizes the processing order of nodes using the specified seed.</p> <p>Arguments: - <code>seed</code>: Randomization seed (must be non-negative)</p>"},{"location":"api/data/#name-randomisation","title":"Name Randomisation","text":""},{"location":"api/data/#randomise_namesseed-optionalint-none","title":"<code>randomise_names(seed: Optional[int]) -&gt; None</code>","text":"<p>Randomizes node names for sensitivity testing. When <code>seed=None</code>, reverts to original names.</p> <p>Arguments: - <code>seed</code>: Randomization seed or None to revert</p>"},{"location":"api/data/#abstract-methods","title":"Abstract Methods","text":"<p>The following methods must be implemented by concrete subclasses:</p>"},{"location":"api/data/#sample-management","title":"Sample Management","text":"<ul> <li><code>set_N(N, seed, random_selection)</code>: Set working sample size with optional randomization</li> <li><code>_update_sample(old_N, old_ext_to_orig)</code>: Update sample after parameter changes</li> </ul>"},{"location":"api/data/#data-access","title":"Data Access","text":"<ul> <li><code>marginals(node, parents, values_reqd)</code>: Get marginal distributions</li> <li><code>values(nodes)</code>: Return values for specified nodes  </li> <li><code>as_df()</code>: Convert to pandas DataFrame representation</li> </ul>"},{"location":"api/data/#persistence","title":"Persistence","text":"<ul> <li><code>write(filename)</code>: Write data to file</li> </ul>"},{"location":"api/data/#properties-from-bnfit","title":"Properties (from BNFit)","text":""},{"location":"api/data/#node-information","title":"Node Information","text":"<ul> <li><code>nodes</code>: Internal (original) node names</li> <li><code>node_types</code>: Node type mapping (e.g., <code>{node1: type1, ...}</code>)</li> <li><code>node_values</code>: Value counts for categorical nodes</li> </ul>"},{"location":"api/data/#sample-information","title":"Sample Information","text":"<ul> <li><code>N</code>: Current sample size being used</li> <li><code>sample</code>: Access to underlying data sample</li> </ul>"},{"location":"api/data/#type-system","title":"Type System","text":""},{"location":"api/data/#datasettype-enumeration","title":"DatasetType Enumeration","text":"<pre><code>class DatasetType(StrEnum):\n    CATEGORICAL = \"categorical\"  # All categorical variables\n    CONTINUOUS = \"continuous\"    # All float variables  \n    MIXED = \"mixed\"             # Mixed categorical and numeric\n</code></pre>"},{"location":"api/data/#variabletype-enumeration","title":"VariableType Enumeration","text":"<pre><code>class VariableType(StrEnum):\n    INT16 = \"int16\"\n    INT32 = \"int32\" \n    INT64 = \"int64\"\n    FLOAT32 = \"float32\"\n    FLOAT64 = \"float64\"\n    CATEGORY = \"category\"\n</code></pre>"},{"location":"api/data/#internal-methods","title":"Internal Methods","text":""},{"location":"api/data/#_set_dstype-none","title":"<code>_set_dstype() -&gt; None</code>","text":"<p>Determines the overall dataset type from individual node types. Sets <code>dstype</code> to:</p> <ul> <li><code>CATEGORICAL</code>: If all variables are categorical</li> <li><code>CONTINUOUS</code>: If all variables are numeric  </li> <li><code>MIXED</code>: If variables are mixed types</li> </ul>"},{"location":"api/data/#_generate_random_namesseed-optionalint-none","title":"<code>_generate_random_names(seed: Optional[int]) -&gt; None</code>","text":"<p>Generates randomized external names for nodes using the format <code>X###NNNNNN</code> where:</p> <ul> <li><code>###</code> is a zero-padded random integer</li> <li><code>NNNNNN</code> is the first 6 characters of the original name</li> </ul> <p>When <code>seed=None</code>, reverts mappings back to original names.</p>"},{"location":"api/data/#usage-patterns","title":"Usage Patterns","text":""},{"location":"api/data/#basic-inheritance","title":"Basic Inheritance","text":"<pre><code>class CustomData(Data):\n    def __init__(self, source):\n        super().__init__()\n        # Initialize from source\n\n    def set_N(self, N, seed=None, random_selection=False):\n        # Implement sample size setting\n\n    def marginals(self, node, parents, values_reqd=False):\n        # Implement marginal computation\n\n    # ... implement other abstract methods\n</code></pre>"},{"location":"api/data/#name-randomization-workflow","title":"Name Randomization Workflow","text":"<pre><code># Store original order for later restoration\noriginal_order = data.get_order()\n\n# Randomize names for sensitivity testing\ndata.randomise_names(seed=42)\n\n# Run algorithm with randomized names\nresults_randomized = run_algorithm(data)\n\n# Restore original names\ndata.randomise_names(seed=None)\n\n# Compare results\ncompare_sensitivity(results_original, results_randomized)\n</code></pre>"},{"location":"api/data/#integration-with-bnfit","title":"Integration with BNFit","text":"<p>The Data class fully implements the BNFit interface, enabling seamless integration with causaliq-core components:</p> <ul> <li>Parameter Estimation: Compatible with BN parameter fitting algorithms</li> <li>Score Calculation: Provides necessary marginals for score-based learning</li> <li>Constraint Testing: Supports conditional independence testing workflows</li> </ul>"},{"location":"api/indep/","title":"Independence Testing - Probabilistic Independence Tests","text":"<p>The <code>indep</code> module provides comprehensive statistical independence testing functionality for causal discovery workflows. It implements multiple test statistics and supports both conditional and unconditional independence testing on data or Bayesian Network parameters.</p>"},{"location":"api/indep/#overview","title":"Overview","text":"<p>Independence tests are fundamental to constraint-based causal discovery algorithms (PC, FCI, etc.) and structure learning validation. The module supports:</p> <ul> <li>Multiple Test Statistics: Chi-squared (X\u00b2) and Mutual Information (MI) tests</li> <li>Conditional Independence: Testing X \u22a5 Y | Z with arbitrary conditioning sets</li> <li>Flexible Data Sources: Works with pandas DataFrames, data files, or BN parameters</li> <li>Comprehensive Validation: Robust argument checking and error handling</li> </ul>"},{"location":"api/indep/#constants","title":"Constants","text":"<pre><code>TESTS = [\"mi\", \"x2\"]  # Supported test types\nTEST_PARAMS = {\"alpha\": 0.05}  # Default test parameters  \nMIN_P_VALUE = 1e-30  # Minimum p-value threshold\n</code></pre>"},{"location":"api/indep/#core-functions","title":"Core Functions","text":""},{"location":"api/indep/#indepx-y-z-data-bnnone-n1000000000-typesmi-dataframe","title":"<code>indep(x, y, z, data, bn=None, N=1000000000, types=\"mi\") -&gt; DataFrame</code>","text":"<p>The main independence testing function that performs statistical tests to determine if variables x and y are independent, optionally conditional on variables z.</p> <p>Arguments:</p> <ul> <li><code>x</code>: Name of the first variable (string)</li> <li><code>y</code>: Name of the second variable (string) </li> <li><code>z</code>: Name(s) of conditioning variables (string, list of strings, or None)</li> <li><code>data</code>: Data source (pandas DataFrame, file path string, or None)</li> <li><code>bn</code>: Bayesian Network for synthetic testing (BN object or None) </li> <li><code>N</code>: Sample size when using BN parameters (int, default: 1,000,000,000)</li> <li><code>types</code>: Test statistic type(s) to compute (string or list, default: \"mi\")</li> </ul> <p>Returns:</p> <p>DataFrame with independence test results where:</p> <ul> <li>Columns: Test types (e.g., \"mi\", \"x2\")</li> <li>Rows: Test statistics (\"statistic\", \"df\", \"p_value\")</li> </ul> <p>Test Statistics:</p> <ul> <li>Chi-squared (\"x2\"): Classical Pearson chi-squared test for independence</li> <li>Formula: \u03a3((observed - expected)\u00b2 / expected)</li> <li> <p>Asymptotically \u03c7\u00b2 distributed under null hypothesis of independence</p> </li> <li> <p>Mutual Information (\"mi\"): Information-theoretic measure of dependence  </p> </li> <li>Formula: 2 \u00d7 \u03a3(observed \u00d7 log(observed / expected))</li> <li>Asymptotically \u03c7\u00b2 distributed (G-test statistic)</li> </ul> <p>Usage Examples:</p> <pre><code>from causaliq_data.indep import indep\nimport pandas as pd\n\n# Load data\ndata = pd.read_csv(\"dataset.csv\")\n\n# Unconditional independence test\nresult = indep(\"X\", \"Y\", None, data, types=\"x2\")\nprint(f\"p-value: {result.loc['p_value', 'x2']}\")\n\n# Conditional independence test  \nresult = indep(\"X\", \"Y\", [\"Z1\", \"Z2\"], data, types=[\"mi\", \"x2\"])\n\n# Test using BN parameters\nfrom causaliq_core.bn.io import read_bn\nbn = read_bn(\"network.dsc\")\nresult = indep(\"A\", \"B\", \"C\", None, bn=bn, N=10000)\n</code></pre> <p>Data Source Options:</p> <ol> <li>DataFrame: Direct pandas DataFrame input</li> <li>File Path: Path to CSV file (loaded automatically)</li> <li>BN Parameters: Uses conditional probability tables from Bayesian Network</li> </ol>"},{"location":"api/indep/#check_test_paramsparams-dictstr-any","title":"<code>check_test_params(params) -&gt; Dict[str, Any]</code>","text":"<p>Validates and standardizes independence test parameters.</p> <p>Arguments:</p> <ul> <li><code>params</code>: Dictionary of test parameters to validate</li> </ul> <p>Supported Parameters:</p> <ul> <li><code>alpha</code>: Significance level for tests (float, 0 &lt; alpha &lt; 1, default: 0.05)</li> </ul> <p>Returns:</p> <p>Dictionary of validated parameters with defaults applied.</p> <p>Raises:</p> <ul> <li><code>TypeError</code>: If parameters have incorrect types</li> <li><code>ValueError</code>: If parameter values are invalid</li> </ul> <p>Example:</p> <pre><code>from causaliq_data.indep import check_test_params\n\n# Validate custom parameters\nparams = check_test_params({\"alpha\": 0.01})\nprint(params)  # {\"alpha\": 0.01}\n\n# Apply defaults\nparams = check_test_params({})  \nprint(params)  # {\"alpha\": 0.05}\n</code></pre>"},{"location":"api/indep/#internal-functions","title":"Internal Functions","text":""},{"location":"api/indep/#check_indep_args-tuple","title":"<code>check_indep_args(...) -&gt; Tuple[...]</code>","text":"<p>Internal function that validates and standardizes all arguments for independence tests.</p> <p>Key Validations:</p> <ul> <li>Type checking for all arguments</li> <li>Variable name uniqueness</li> <li>Data/BN consistency checks</li> <li>Column/node existence validation  </li> <li>Sample size validation</li> </ul>"},{"location":"api/indep/#_statisticactuals-type-tupleint-float","title":"<code>_statistic(actuals, type) -&gt; Tuple[int, float]</code>","text":"<p>Internal function that computes test statistics from contingency tables.</p> <p>Arguments:</p> <ul> <li><code>actuals</code>: 2D list representing contingency table counts</li> <li><code>type</code>: Test statistic type (\"x2\" or \"mi\")</li> </ul> <p>Returns:</p> <p>Tuple of (degrees of freedom, test statistic value)</p> <p>Features:</p> <ul> <li>Handles zero-count tables (returns 0.0 statistic)</li> <li>Robust error handling for malformed inputs</li> <li>Optimized computation for both test types</li> </ul>"},{"location":"api/indep/#statistical-details","title":"Statistical Details","text":""},{"location":"api/indep/#test-assumptions","title":"Test Assumptions","text":"<p>Both chi-squared and mutual information tests assume:</p> <ol> <li>Categorical Variables: All variables must be discrete/categorical</li> <li>Sufficient Sample Size: Large enough samples for asymptotic properties</li> <li>Independent Observations: Rows represent independent samples</li> <li>No Missing Data: Complete case analysis only</li> </ol>"},{"location":"api/indep/#degrees-of-freedom","title":"Degrees of Freedom","text":"<p>For contingency tables with dimensions r \u00d7 c:</p> <ul> <li>Degrees of Freedom: (r-1) \u00d7 (c-1)</li> <li>Conditional Tests: Sum across conditioning set combinations</li> </ul>"},{"location":"api/indep/#p-value-computation","title":"P-value Computation","text":"<p>P-values are computed using the chi-squared distribution: <pre><code>p_value = 1.0 - stats.chi2.cdf(statistic, degrees_of_freedom)\n</code></pre></p> <p>Values below <code>MIN_P_VALUE</code> (1e-30) are set to 0.0 for numerical stability.</p>"},{"location":"api/indep/#error-handling","title":"Error Handling","text":"<p>The module provides comprehensive error checking:</p> <p>Type Errors:</p> <ul> <li>Non-string variable names</li> <li>Invalid data types for DataFrame/BN arguments</li> <li>Malformed conditioning sets or test type specifications</li> </ul> <p>Value Errors:</p> <ul> <li>Duplicate variable names</li> <li>Variables not present in data/BN</li> <li>Negative sample sizes</li> <li>Unsupported or duplicate test types</li> <li>Empty test specifications</li> </ul> <p>File Errors:</p> <ul> <li>Missing data files</li> <li>Malformed CSV data</li> </ul>"},{"location":"api/indep/#integration-with-causal-discovery","title":"Integration with Causal Discovery","text":"<p>Independence tests are essential for:</p>"},{"location":"api/indep/#constraint-based-algorithms","title":"Constraint-Based Algorithms","text":"<pre><code># PC Algorithm skeleton discovery\nif indep(\"X\", \"Y\", [], data)[\"mi\"][\"p_value\"] &gt; 0.05:\n    # Remove edge X-Y\n    pass\n\n# Conditional independence for orientation  \nif indep(\"X\", \"Y\", [\"Z\"], data)[\"mi\"][\"p_value\"] &lt;= 0.05:\n    # Orient edge based on dependence\n    pass\n</code></pre>"},{"location":"api/indep/#structure-learning-validation","title":"Structure Learning Validation","text":"<pre><code># Validate learned structure\nlearned_bn = learn_structure(data)\nfor x, y in learned_bn.edges():\n    parents_xy = list(set(learned_bn.parents(x) + learned_bn.parents(y)))\n    test_result = indep(x, y, parents_xy, data)\n    if test_result[\"mi\"][\"p_value\"] &gt; 0.05:\n        print(f\"Warning: {x}-{y} may be spurious\")\n</code></pre>"},{"location":"api/indep/#synthetic-data-validation","title":"Synthetic Data Validation","text":"<pre><code># Test independence properties in generated data\ntrue_bn = read_bn(\"true_network.dsc\")\nsynthetic_data = generate_data(true_bn, N=5000)\n\n# Verify independence assumptions hold\nfor node in true_bn.nodes:\n    non_descendants = true_bn.non_descendants(node)\n    for nd in non_descendants:\n        parents = true_bn.parents(node)\n        result = indep(node, nd, parents, synthetic_data)\n        assert result[\"mi\"][\"p_value\"] &gt; 0.05, f\"{node} should be independent of {nd}\"\n</code></pre>"},{"location":"api/indep/#performance-considerations","title":"Performance Considerations","text":""},{"location":"api/indep/#computational-complexity","title":"Computational Complexity","text":"<ul> <li>Contingency Table Construction: O(n \u00d7 k) where n = sample size, k = variables</li> <li>Statistic Computation: O(r \u00d7 c) where r, c are table dimensions  </li> <li>Conditional Tests: Multiplicative in conditioning set size</li> </ul>"},{"location":"api/indep/#memory-usage","title":"Memory Usage","text":"<ul> <li>Sparse Tables: Efficient handling of sparse contingency tables</li> <li>Batch Processing: Processes all conditioning combinations efficiently</li> <li>Memory Reuse: Minimal copying in DataFrame operations</li> </ul>"},{"location":"api/indep/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Batch Multiple Tests: Use <code>types=[\"mi\", \"x2\"]</code> for multiple statistics</li> <li>Limit Conditioning Sets: Large conditioning sets increase computational cost</li> <li>Sample Size Management: Use appropriate N values with BN parameters</li> <li>Data Preprocessing: Pre-filter and clean data before testing</li> </ol>"},{"location":"api/indep/#see-also","title":"See Also","text":"<ul> <li>Data: Base data handling interface</li> <li>Pandas: DataFrame-based data adapter  </li> <li>Score: Structure scoring functions</li> <li>CausalIQ Core: Bayesian Network and DAG functionality</li> </ul>"},{"location":"api/numpy/","title":"NumPy - High-Performance Array Adapter","text":"<p>The <code>NumPy</code> class provides a high-performance implementation of the Data interface using NumPy arrays as the underlying storage. This adapter is optimized for computational efficiency and large-scale causal discovery operations.</p>"},{"location":"api/numpy/#class-definition","title":"Class Definition","text":"<pre><code>class NumPy(Data):\n    \"\"\"Concrete Data subclass which holds data in NumPy arrays.\n\n    Args:\n        data (ndarray): Data provided as a 2-D NumPy array.\n        dstype (DatasetType): Type of variables in dataset.\n        col_values (dict): Column names and their categorical values.\n\n    Attributes:\n        data (ndarray): The original data values.\n        sample (ndarray): Sample values of size N, rows possibly reordered.\n        categories: Categories for each categorical node.\n    \"\"\"\n</code></pre>"},{"location":"api/numpy/#constructor","title":"Constructor","text":""},{"location":"api/numpy/#__init__data-dstype-col_valuesnone-none","title":"<code>__init__(data, dstype, col_values=None) -&gt; None</code>","text":"<p>Creates a NumPy data adapter from a 2D NumPy array.</p> <p>Arguments: - <code>data</code>: 2D NumPy array with shape (n_samples, n_features) - <code>dstype</code>: Dataset type (<code>DatasetType.CATEGORICAL</code>, <code>CONTINUOUS</code>, or <code>MIXED</code>) - <code>col_values</code>: Optional mapping of column names to categorical values</p> <p>Validation: - Data must be 2D NumPy array - Minimum 2 samples and 2 features required - For categorical data, values must be integer-encoded starting from 0</p> <p>Initialization: - Sets up node names as X0, X1, X2, ... by default - Converts categorical values to appropriate categories - Determines node types based on dstype</p>"},{"location":"api/numpy/#factory-methods","title":"Factory Methods","text":""},{"location":"api/numpy/#from_dfdf-dstype-keep_dffalse-numpy","title":"<code>from_df(df, dstype, keep_df=False) -&gt; 'NumPy'</code>","text":"<p>Creates a NumPy instance from a pandas DataFrame.</p> <p>Arguments: - <code>df</code>: Pandas DataFrame containing the data - <code>dstype</code>: Target dataset type for conversion - <code>keep_df</code>: Whether to preserve DataFrame for <code>as_df()</code> operations</p> <p>Features: - Automatic conversion from pandas to NumPy format - Intelligent handling of categorical data encoding - Optional DataFrame preservation for round-trip compatibility</p> <p>Example: <pre><code># Convert from Pandas\npandas_data = Pandas.read(\"data.csv\", dstype=\"categorical\")\nnumpy_data = NumPy.from_df(pandas_data.as_df(), \n                          dstype=\"categorical\", \n                          keep_df=True)\n</code></pre></p>"},{"location":"api/numpy/#high-performance-operations","title":"High-Performance Operations","text":""},{"location":"api/numpy/#set_nn-seednone-random_selectionfalse-none","title":"<code>set_N(N, seed=None, random_selection=False) -&gt; None</code>","text":"<p>Sets working sample size with optimized sampling strategies.</p> <p>Arguments:</p> <ul> <li><code>N</code>: Target sample size</li> <li><code>seed</code>: Random seed for reproducible results</li> <li><code>random_selection</code>: Use random subset vs first N rows</li> </ul> <p>Performance Features:</p> <ul> <li>Random Selection: Uses <code>numpy.random.choice()</code> for efficient random sampling</li> <li>Row Shuffling: Optional in-place shuffling with <code>permutation()</code></li> <li>Memory Optimization: Works with array views when possible</li> <li>Type Conversion: Converts continuous data to float64 for precision only when needed</li> </ul> <p>Implementation Details: <pre><code># Efficient random selection without replacement\nindices = rng.choice(self.data.shape[0], size=N, replace=False)\nself._sample = self.data[indices if seed != 0 else sorted(indices)]\n\n# In-place row order randomization\nif seed is not None and seed != 0:\n    order = rng.permutation(N)\n    self._sample = self.sample[order]\n</code></pre></p>"},{"location":"api/numpy/#uniquej_reqd-num_vals-tuplendarray-ndarray","title":"<code>unique(j_reqd, num_vals) -&gt; Tuple[ndarray, ndarray]</code>","text":"<p>Highly optimized unique value detection and counting.</p> <p>Arguments:</p> <ul> <li><code>j_reqd</code>: Tuple of column indices for which unique combinations are needed</li> <li><code>num_vals</code>: Array of number of unique values per column</li> </ul> <p>Returns:</p> <ul> <li><code>(combinations, counts)</code>: Unique value combinations and their frequencies</li> </ul> <p>Optimization Strategy: <pre><code># Fast path for small combination spaces\nmax_combinations = prod(num_vals)\nif max_combinations &lt;= THRESHOLD:\n    # Use integer packing for ultra-fast counting\n    # Pack multiple values into single integers\n    multipliers = [prod(num_vals[i+1:]) for i in range(len(num_vals))]\n    packed = dot(self.sample[:, j_reqd], multipliers)\n    counts = bincount(packed)\n    # Unpack results efficiently\nelse:\n    # Fall back to numpy.unique for large spaces\n    combos, counts = npunique(self.sample[:, j_reqd], \n                             axis=0, return_counts=True)\n</code></pre></p>"},{"location":"api/numpy/#in-memory-counting-optimizations","title":"In-Memory Counting Optimizations","text":""},{"location":"api/numpy/#categorical-value-counting","title":"Categorical Value Counting","text":"<pre><code># Ultra-fast categorical counting using bincount\nfor j in range(self.sample.shape[1]):\n    counts = {\n        self.categories[j][v]: c \n        for v, c in enumerate(bincount(self.sample[:, j]))\n    }\n    self._node_values[node_name] = {v: counts[v] for v in sorted(counts)}\n</code></pre>"},{"location":"api/numpy/#memory-efficient-storage","title":"Memory-Efficient Storage","text":"<ul> <li>Uses minimal integer types for categorical data (typically int16 or int32)</li> <li>Lazy conversion to float64 only for continuous scoring operations</li> <li>Strategic copying vs view usage to minimize memory footprint</li> </ul>"},{"location":"api/numpy/#advanced-sampling","title":"Advanced Sampling","text":""},{"location":"api/numpy/#random-selection-strategies","title":"Random Selection Strategies","text":"<p>Random Subset Selection: <pre><code>data.set_N(1000, seed=42, random_selection=True)\n# Randomly selects 1000 rows from dataset\n</code></pre></p> <p>Ordered Sampling with Shuffling: <pre><code>data.set_N(1000, seed=42, random_selection=False)\n# Uses first 1000 rows but randomizes their order\n</code></pre></p>"},{"location":"api/numpy/#deterministic-reproducibility","title":"Deterministic Reproducibility","text":"<ul> <li>Seed=0 and seed=None both preserve original data order</li> <li>Positive seeds enable reproducible randomization</li> <li>Consistent behavior across multiple calls with same seed</li> </ul>"},{"location":"api/numpy/#statistical-operations","title":"Statistical Operations","text":""},{"location":"api/numpy/#marginalsnode-parents-values_reqdfalse-tuple","title":"<code>marginals(node, parents, values_reqd=False) -&gt; Tuple</code>","text":"<p>Efficient marginal computation using NumPy operations.</p> <p>Implementation:</p> <ul> <li>Leverages optimized <code>unique()</code> method for counting</li> <li>Handles sparse parent configurations efficiently</li> <li>Returns results in format compatible with scoring algorithms</li> </ul>"},{"location":"api/numpy/#valuesnodes-ndarray","title":"<code>values(nodes) -&gt; ndarray</code>","text":"<p>Direct array access for specified columns.</p> <p>Performance:</p> <ul> <li>Returns views when possible to avoid copying</li> <li>Maintains column order as specified</li> <li>Efficient slicing for subset access</li> </ul>"},{"location":"api/numpy/#memory-management","title":"Memory Management","text":""},{"location":"api/numpy/#data-storage-strategy","title":"Data Storage Strategy","text":"<pre><code>self.data        # Original immutable data\nself._sample     # Current working sample (possibly reordered)\nself.categories  # Categorical value mappings (shared across samples)\n</code></pre>"},{"location":"api/numpy/#copy-on-write-semantics","title":"Copy-on-Write Semantics","text":"<ul> <li>Original data never modified</li> <li>Sample arrays created as views when order unchanged</li> <li>Copies created only when shuffling or subset selection required</li> </ul>"},{"location":"api/numpy/#type-optimization","title":"Type Optimization","text":"<ul> <li>Categorical data stored as smallest possible integer type</li> <li>Continuous data uses float32 by default, converted to float64 only for scoring</li> <li>Automatic type inference minimizes memory usage</li> </ul>"},{"location":"api/numpy/#name-randomization","title":"Name Randomization","text":""},{"location":"api/numpy/#randomise_namesseednone-none","title":"<code>randomise_names(seed=None) -&gt; None</code>","text":"<p>Efficient node name randomization without data copying.</p> <p>Features:</p> <ul> <li>Updates only mapping dictionaries, not underlying arrays</li> <li>Preserves all data relationships and types</li> <li>Updates cached node_values and node_types dictionaries consistently</li> </ul>"},{"location":"api/numpy/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"api/numpy/#typical-performance-characteristics","title":"Typical Performance Characteristics","text":"<p>Memory Usage:</p> <ul> <li>~50-80% less memory than equivalent pandas DataFrame</li> <li>Categorical data: ~2-4 bytes per value vs 8+ bytes in pandas</li> <li>Continuous data: 4 bytes (float32) vs 8 bytes (float64) by default</li> </ul> <p>Computational Speed:</p> <ul> <li>Unique value detection: 10-100x faster than pandas for categorical data</li> <li>Sample subset creation: 5-20x faster than DataFrame operations</li> <li>Marginal calculations: 20-50x faster for large datasets</li> </ul> <p>Scalability:</p> <ul> <li>Efficiently handles datasets with millions of rows</li> <li>Linear scaling with data size for most operations</li> <li>Memory usage scales predictably with dataset dimensions</li> </ul>"},{"location":"api/numpy/#integration-examples","title":"Integration Examples","text":""},{"location":"api/numpy/#high-performance-workflow","title":"High-Performance Workflow","text":"<pre><code># Load and convert for performance\npandas_data = Pandas.read(\"large_dataset.csv\", dstype=\"categorical\")\nnumpy_data = NumPy.from_df(pandas_data.as_df(), \n                          dstype=\"categorical\")\n\n# Set large working sample efficiently\nnumpy_data.set_N(100000, seed=42, random_selection=True)\n\n# Perform intensive causal discovery\nresults = heavy_computation_algorithm(numpy_data)\n</code></pre>"},{"location":"api/numpy/#memory-conscious-processing","title":"Memory-Conscious Processing","text":"<pre><code># Process data in chunks for memory efficiency\nfor chunk_seed in range(10):\n    numpy_data.set_N(10000, seed=chunk_seed, random_selection=True)\n    chunk_results = process_chunk(numpy_data)\n    aggregate_results(chunk_results)\n</code></pre>"},{"location":"api/numpy/#benchmarking-and-experimentation","title":"Benchmarking and Experimentation","text":"<pre><code># Performance comparison across randomizations\ntiming_results = []\nfor trial in range(100):\n    numpy_data.randomise_order(seed=trial)\n    start_time = time.time()\n    result = algorithm.run(numpy_data)\n    timing_results.append(time.time() - start_time)\n\nprint(f\"Mean runtime: {np.mean(timing_results):.3f}s\")\nprint(f\"Std deviation: {np.std(timing_results):.3f}s\")\n</code></pre>"},{"location":"api/numpy/#best-practices","title":"Best Practices","text":""},{"location":"api/numpy/#when-to-use-numpy-adapter","title":"When to Use NumPy Adapter","text":"<ul> <li>Large datasets (&gt;10K rows typically)</li> <li>Performance-critical causal discovery algorithms</li> <li>Memory-constrained environments</li> <li>Repeated statistical computations</li> <li>Benchmark and stability experiments requiring many randomizations</li> </ul>"},{"location":"api/numpy/#optimization-tips","title":"Optimization Tips","text":"<ul> <li>Use <code>random_selection=True</code> only when needed (creates copy)</li> <li>Convert from Pandas early in pipeline for consistent performance</li> <li>Leverage <code>keep_df=True</code> only if round-trip DataFrame access needed</li> <li>Choose appropriate <code>dstype</code> for your data characteristics</li> </ul>"},{"location":"api/oracle/","title":"Oracle - Synthetic Data Generator","text":"<p>The <code>Oracle</code> class provides a specialized data adapter that generates synthetic data from known Bayesian Networks. This adapter is primarily used for algorithm validation, benchmarking, and controlled experiments where the true underlying causal structure is known.</p>"},{"location":"api/oracle/#class-definition","title":"Class Definition","text":"<pre><code>class Oracle(Data):\n    \"\"\"Oracle data adapter for synthetic data generation from Bayesian Networks.\n\n    Args:\n        bn: A BN (Bayesian Network) object from causaliq-core.\n\n    Attributes:\n        bn: The underlying Bayesian Network object.\n    \"\"\"\n</code></pre>"},{"location":"api/oracle/#constructor","title":"Constructor","text":""},{"location":"api/oracle/#__init__bn-none","title":"<code>__init__(bn) -&gt; None</code>","text":"<p>Creates an Oracle data adapter from a Bayesian Network.</p> <p>Arguments: - <code>bn</code>: BN object from causaliq-core containing DAG structure and CPTs</p> <p>Validation: - Input must be a valid BN object - BN must contain both DAG structure and conditional probability tables - All nodes must have associated conditional distributions</p> <p>Initialization: - Extracts node names from BN DAG structure - Determines variable types from conditional distributions (CPT vs continuous) - Sets initial sample size to 1 (can be changed with <code>set_N()</code>)</p> <p>Example: <pre><code>from causaliq_core.bn.io import read_bn\nfrom causaliq_data import Oracle\n\n# Load BN from file\nbn = read_bn(\"cancer.dsc\")\n\n# Create Oracle adapter\noracle = Oracle(bn)\nprint(f\"Nodes: {oracle.nodes}\")\nprint(f\"Types: {oracle.node_types}\")\n</code></pre></p>"},{"location":"api/oracle/#synthetic-data-generation","title":"Synthetic Data Generation","text":""},{"location":"api/oracle/#set_nn-seednone-random_selectionfalse-none","title":"<code>set_N(N, seed=None, random_selection=False) -&gt; None</code>","text":"<p>Sets the effective sample size for synthetic data operations.</p> <p>Arguments:</p> <ul> <li><code>N</code>: Target sample size for synthetic data generation</li> <li><code>seed</code>: Must be None (not supported for Oracle)</li> <li><code>random_selection</code>: Must be False (not applicable)</li> </ul> <p>Behavior:</p> <ul> <li>Updates internal sample size counter</li> <li>Does not actually generate data (Oracle provides analytical answers)</li> <li>Used by algorithms to determine confidence/precision of estimates</li> </ul> <p>Validation:</p> <ul> <li><code>N</code> must be positive integer</li> <li><code>seed</code> parameter must be None (raises TypeError if provided)</li> <li><code>random_selection</code> must be False</li> </ul> <p>Usage: <pre><code>oracle.set_N(10000)  # Set effective sample size\nprint(f\"Sample size: {oracle.N}\")\n</code></pre></p>"},{"location":"api/oracle/#statistical-operations","title":"Statistical Operations","text":""},{"location":"api/oracle/#marginalsnode-parents-values_reqdfalse-tuple","title":"<code>marginals(node, parents, values_reqd=False) -&gt; Tuple</code>","text":"<p>Provides exact marginal distributions from the Bayesian Network.</p> <p>Arguments:</p> <ul> <li><code>node</code>: Target node name (internal name)</li> <li><code>parents</code>: Dictionary specifying parent values <code>{parent: value}</code></li> <li><code>values_reqd</code>: Whether to return value labels (always False for Oracle)</li> </ul> <p>Returns:</p> <ul> <li>Exact conditional probability distribution for the node given parents</li> <li>For categorical nodes: probability vector over possible values</li> <li>For continuous nodes: parameters of the conditional distribution</li> </ul> <p>Features:</p> <ul> <li>Exact Results: Returns true probabilities, not empirical estimates</li> <li>No Sampling Error: Results are analytical, not subject to sampling variation</li> <li>Efficient Computation: Leverages BN's internal probability representations</li> </ul> <p>Example: <pre><code># Get P(Cancer | Smoker=True, Pollution=High)\nmarginal = oracle.marginals(\"Cancer\", \n                           {\"Smoker\": \"True\", \"Pollution\": \"High\"})\nprint(f\"P(Cancer=True|evidence): {marginal[0][1]}\")\n</code></pre></p>"},{"location":"api/oracle/#valuesnodes-npndarray","title":"<code>values(nodes) -&gt; np.ndarray</code>","text":"<p>Not Implemented: Oracle does not store actual data values.</p> <p>Raises:</p> <ul> <li><code>TypeError</code>: Always raised with message \"Oracle.values() not implemented\"</li> </ul> <p>Rationale:</p> <ul> <li>Oracle provides analytical results, not sampled data</li> <li>Use concrete adapters (Pandas/NumPy) for data value access</li> <li>Consistent with Oracle's role as synthetic probability source</li> </ul>"},{"location":"api/oracle/#specialized-oracle-features","title":"Specialized Oracle Features","text":""},{"location":"api/oracle/#true-parameter-access","title":"True Parameter Access","text":"<p>Oracle provides direct access to the true parameters of the Bayesian Network:</p> <p>Conditional Probability Tables: <pre><code># Access true CPT for a categorical node\ncpt = oracle.bn.cnds[\"Disease\"]\nprint(\"True conditional probabilities:\")\nfor parent_config in cpt.parents_configs():\n    for value in cpt.values:\n        prob = cpt.get_prob(parent_config, value)\n        print(f\"P({value}|{parent_config}) = {prob}\")\n</code></pre></p> <p>Network Structure: <pre><code># Access true DAG structure\ndag = oracle.bn.dag\nprint(f\"True edges: {dag.edges}\")\nprint(f\"True parents of X: {dag.parents('X')}\")\n</code></pre></p>"},{"location":"api/oracle/#algorithm-validation","title":"Algorithm Validation","text":"<p>Oracle is ideal for validating causal discovery algorithms:</p> <p>Score Validation: <pre><code># Compare algorithm scores with true model\ntrue_score = oracle.score(learned_dag)\noracle_score = oracle.score(oracle.bn.dag)  # True structure score\nprint(f\"Score difference: {abs(true_score - oracle_score)}\")\n</code></pre></p> <p>Conditional Independence Testing: <pre><code># Test algorithm's CI conclusions against true model\nfor x, y, z in ci_tests:\n    true_independent = oracle.bn.d_separated(x, y, z)\n    algorithm_independent = algorithm.ci_test(oracle, x, y, z)\n    accuracy = (true_independent == algorithm_independent)\n</code></pre></p>"},{"location":"api/oracle/#limitations-and-constraints","title":"Limitations and Constraints","text":""},{"location":"api/oracle/#unsupported-operations","title":"Unsupported Operations","text":"<p>Data Value Access: - <code>values()</code> method raises TypeError - No actual data samples available - Use for probability queries only</p> <p>Randomization Restrictions:</p> <ul> <li><code>randomise_names()</code> raises NotImplementedError  </li> <li>Name randomization not meaningful for Oracle</li> <li>Node names tied to BN structure</li> </ul> <p>Sampling Limitations:</p> <ul> <li>No row-level sampling or shuffling</li> <li><code>set_N()</code> only affects effective sample size for algorithms</li> <li>No actual data generation performed</li> </ul>"},{"location":"api/oracle/#data-type-constraints","title":"Data Type Constraints","text":"<p>Variable Types:</p> <ul> <li>Categorical variables: Must have finite discrete values</li> <li>Continuous variables: Limited to supported distribution types</li> <li>Mixed networks: Handled according to individual node types</li> </ul>"},{"location":"api/oracle/#integration-with-causaliq-ecosystem","title":"Integration with CausalIQ Ecosystem","text":""},{"location":"api/oracle/#algorithm-testing-framework","title":"Algorithm Testing Framework","text":"<pre><code>def test_algorithm_accuracy(algorithm, test_bns):\n    results = []\n    for bn_file in test_bns:\n        # Load true BN\n        bn = read_bn(bn_file)\n        oracle = Oracle(bn)\n\n        # Run algorithm\n        oracle.set_N(10000)  # Large effective sample size\n        learned_structure = algorithm.run(oracle)\n\n        # Compare with true structure  \n        accuracy = compare_structures(bn.dag, learned_structure)\n        results.append(accuracy)\n\n    return results\n</code></pre>"},{"location":"api/oracle/#benchmark-experiments","title":"Benchmark Experiments","text":"<pre><code>def benchmark_scoring_functions(oracle, scoring_functions):\n    true_score = {}\n    for score_fn in scoring_functions:\n        # Get score for true structure\n        true_score[score_fn.name] = score_fn.calculate(oracle, oracle.bn.dag)\n\n        # Test alternative structures\n        for alt_structure in generate_alternatives(oracle.bn.dag):\n            alt_score = score_fn.calculate(oracle, alt_structure)\n            print(f\"{score_fn.name}: True={true_score[score_fn.name]:.3f}, \"\n                  f\"Alt={alt_score:.3f}\")\n</code></pre>"},{"location":"api/oracle/#stability-analysis","title":"Stability Analysis","text":"<pre><code>def analyze_algorithm_stability(algorithm, oracle, trials=100):\n    # Oracle provides consistent \"data\" across trials\n    results = []\n    for trial in range(trials):\n        oracle.randomise_order(trial)  # Change processing order\n        result = algorithm.run(oracle)\n        results.append(result)\n\n    # Analyze consistency of results\n    return assess_stability(results)\n</code></pre>"},{"location":"api/oracle/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"api/oracle/#computational-efficiency","title":"Computational Efficiency","text":"<ul> <li>Analytical Operations: No sampling or counting required</li> <li>Exact Computations: Probability queries return exact values</li> <li>Memory Efficient: No large data arrays stored</li> <li>Fast Initialization: Only stores BN structure and parameters</li> </ul>"},{"location":"api/oracle/#scalability","title":"Scalability","text":"<ul> <li>Network Size: Performance depends on BN complexity, not sample size</li> <li>Query Complexity: Marginal queries scale with network connectivity</li> <li>Memory Usage: Minimal, proportional to BN size only</li> </ul>"},{"location":"api/oracle/#best-practices","title":"Best Practices","text":""},{"location":"api/oracle/#when-to-use-oracle","title":"When to Use Oracle","text":"<ul> <li>Algorithm Validation: Testing against known ground truth</li> <li>Benchmarking: Comparing algorithm performance across known structures</li> <li>Method Development: Developing new algorithms with reliable test cases</li> <li>Educational Use: Demonstrating causal discovery concepts</li> </ul>"},{"location":"api/oracle/#usage-patterns","title":"Usage Patterns","text":"<pre><code># Validation workflow\noracle = Oracle(known_bn)\noracle.set_N(sample_size)\n\n# Test your algorithm\nlearned_result = your_algorithm.discover(oracle)\n\n# Compare with truth\naccuracy_metrics = evaluate_against_truth(learned_result, oracle.bn)\n</code></pre>"},{"location":"api/oracle/#integration-tips","title":"Integration Tips","text":"<ul> <li>Use Oracle early in algorithm development for debugging</li> <li>Combine with Pandas/NumPy adapters for comprehensive testing</li> <li>Leverage exact probabilities for theoretical analysis</li> <li>Document true structure properties for result interpretation</li> </ul>"},{"location":"api/overview/","title":"CausalIQ Data API Reference","text":"<p>The CausalIQ Data API provides a unified interface for data handling in causal discovery workflows. The API is built around a plug-in architecture with concrete implementations for different data backends.</p>"},{"location":"api/overview/#core-design","title":"Core Design","text":"<p>All data adapters implement the <code>Data</code> abstract base class, which extends the BNFit interface from causaliq-core. This ensures consistent behavior across different data sources while allowing backend-specific optimizations.</p>"},{"location":"api/overview/#module-structure","title":"Module Structure","text":""},{"location":"api/overview/#data-abstract-base-class","title":"<code>Data</code> - Abstract Base Class","text":"<p>The foundational abstract class that defines the core interface for all data adapters. Provides:</p> <ul> <li>Node Management: Consistent handling of variable names and ordering</li> <li>Randomisation Framework: Built-in support for data and name randomisation </li> <li>BNFit Interface: Full compatibility with Bayesian Network fitting operations</li> <li>Type System: Unified variable type handling across data sources</li> </ul>"},{"location":"api/overview/#pandas-dataframe-based-adapter","title":"<code>Pandas</code> - DataFrame-Based Adapter","text":"<p>A concrete implementation that wraps pandas DataFrames for flexible data handling:</p> <ul> <li>Rich Type Support: Native pandas categorical and numeric types</li> <li>File I/O: Direct CSV reading with compression support</li> <li>Data Validation: Comprehensive missing data and type checking</li> <li>Memory Efficiency: Smart sampling and subsetting without data duplication</li> </ul>"},{"location":"api/overview/#numpy-high-performance-array-adapter","title":"<code>NumPy</code> - High-Performance Array Adapter","text":"<p>A high-performance implementation using NumPy arrays for computational efficiency:</p> <ul> <li>Optimized Counting: Fast categorical data counting using <code>bincount</code></li> <li>Memory Management: Efficient handling of large datasets with minimal copying</li> <li>Type Optimization: Automatic selection of appropriate numeric types</li> <li>Advanced Sampling: Multiple strategies for data subset selection and randomization</li> </ul>"},{"location":"api/overview/#oracle-synthetic-data-generator","title":"<code>Oracle</code> - Synthetic Data Generator","text":"<p>A specialized adapter for generating synthetic data from known Bayesian Networks:</p> <ul> <li>BN Integration: Direct integration with causaliq-core BN objects</li> <li>Parameter Access: Direct access to true conditional probability tables</li> <li>Testing Support: Ideal for algorithm validation and benchmarking</li> <li>Simulation Control: Flexible sample size management for experiments</li> </ul>"},{"location":"api/overview/#score-scoring-functions-for-causal-structure-learning","title":"<code>Score</code> - Scoring Functions for Causal Structure Learning","text":"<p>A comprehensive module providing scoring functions for evaluating Bayesian networks and DAGs:</p> <ul> <li>Multiple Score Types: Support for entropy-based, Bayesian, and Gaussian scoring methods</li> <li>Categorical Scoring: BIC, AIC, log-likelihood, BDE, K2, and other Bayesian scores</li> <li>Gaussian Scoring: BGE, Gaussian BIC, and Gaussian log-likelihood for continuous data</li> <li>Network Evaluation: Complete DAG and Bayesian Network scoring with per-node breakdowns</li> <li>Parameter Validation: Automatic parameter checking and default value assignment</li> </ul>"},{"location":"api/overview/#independence-testing-probabilistic-independence-tests","title":"<code>Independence Testing</code> - Probabilistic Independence Tests","text":"<p>Statistical independence testing functionality for constraint-based causal discovery:</p> <ul> <li>Multiple Test Statistics: Chi-squared (X\u00b2) and Mutual Information (MI) tests</li> <li>Conditional Independence: Support for testing X \u22a5 Y | Z with arbitrary conditioning sets</li> <li>Flexible Data Sources: Works with pandas DataFrames, data files, or Bayesian Network parameters</li> <li>Robust Validation: Comprehensive argument checking and error handling</li> <li>Integration Ready: Designed for use in PC, FCI, and other constraint-based algorithms</li> </ul>"},{"location":"api/overview/#preprocess-data-preprocessing-utilities","title":"<code>Preprocess</code> - Data Preprocessing Utilities","text":"<p>Data cleaning and preparation utilities for Bayesian Network workflows:</p> <ul> <li>Single-Valued Variable Removal: Automatic detection and removal of constant variables</li> <li>Network Restructuring: Intelligent BN reconstruction after variable removal</li> <li>Data Validation: Ensures minimum variable requirements for meaningful analysis</li> <li>Categorical Optimization: Proper type handling for downstream operations</li> <li>Integration Support: Seamless workflow integration with data adapters and structure learning</li> </ul>"},{"location":"api/overview/#common-patterns","title":"Common Patterns","text":""},{"location":"api/overview/#data-loading","title":"Data Loading","text":"<pre><code>from causaliq_data import Pandas, NumPy\n\n# Load from CSV file\ndata = Pandas.read(\"dataset.csv\", dstype=\"categorical\")\n\n# Convert to NumPy for performance\nnumpy_data = NumPy.from_df(data.as_df(), dstype=\"categorical\")\n</code></pre>"},{"location":"api/overview/#randomisation-workflows","title":"Randomisation Workflows","text":"<pre><code># Randomize node names for sensitivity testing\ndata.randomise_names(seed=42)\n\n# Randomize processing order\ndata.randomise_order(seed=123)\n\n# Set working sample size with randomization\ndata.set_N(1000, seed=456, random_selection=True)\n</code></pre>"},{"location":"api/overview/#statistical-operations","title":"Statistical Operations","text":"<pre><code># Get marginal distributions\nmarginals = data.marginals(\"target_node\", {\"parent1\": 0, \"parent2\": 1})\n\n# Access value counts for categorical variables\ncounts = data.node_values[\"categorical_var\"]\n\n# Get unique value combinations\nunique_vals, counts = data.unique((\"var1\", \"var2\"), num_vals)\n</code></pre>"},{"location":"api/overview/#type-system","title":"Type System","text":"<p>The API supports a comprehensive type system for different variable types:</p> <ul> <li>Categorical: <code>VariableType.CATEGORY</code> for discrete variables</li> <li>Integers: <code>INT16</code>, <code>INT32</code>, <code>INT64</code> for integer data</li> <li>Floats: <code>FLOAT32</code>, <code>FLOAT64</code> for continuous variables</li> </ul> <p>Dataset types are automatically inferred:</p> <ul> <li>Categorical: All variables are categorical</li> <li>Continuous: All variables are numeric</li> <li>Mixed: Combination of categorical and numeric variables</li> </ul>"},{"location":"api/overview/#performance-considerations","title":"Performance Considerations","text":""},{"location":"api/overview/#memory-efficiency","title":"Memory Efficiency","text":"<ul> <li>Original data is preserved separately from working samples</li> <li>Lazy evaluation of expensive operations</li> <li>Strategic use of data views vs copies</li> </ul>"},{"location":"api/overview/#computational-optimization","title":"Computational Optimization","text":"<ul> <li>NumPy adapter provides the best performance for large datasets</li> <li>Optimized algorithms for unique value detection and counting</li> <li>Efficient handling of categorical data through integer encoding</li> </ul>"},{"location":"api/overview/#scalability","title":"Scalability","text":"<ul> <li>Support for working with data subsets without loading entire datasets</li> <li>Memory-conscious type selection based on data characteristics</li> <li>Configurable thresholds for algorithm selection</li> </ul>"},{"location":"api/overview/#error-handling","title":"Error Handling","text":"<p>All adapters provide comprehensive error handling with descriptive messages:</p> <ul> <li>Type Validation: Strict checking of argument types and values</li> <li>Data Validation: Detection of missing data, invalid formats, and size constraints</li> <li>State Consistency: Validation of internal state consistency across operations</li> </ul>"},{"location":"api/pandas/","title":"Pandas - DataFrame-Based Data Adapter","text":"<p>The <code>Pandas</code> class provides a concrete implementation of the Data interface using pandas DataFrames as the underlying data storage. This adapter is ideal for exploratory data analysis and moderate-sized datasets where pandas' rich functionality is beneficial.</p>"},{"location":"api/pandas/#class-definition","title":"Class Definition","text":"<pre><code>class Pandas(Data):\n    \"\"\"Data subclass which holds data in a Pandas dataframe.\n\n    Args:\n        df: Data provided as a Pandas dataframe.\n\n    Attributes:\n        df: Original Pandas dataframe providing data.\n        dstype: Type of dataset (categorical/numeric/mixed).\n    \"\"\"\n</code></pre>"},{"location":"api/pandas/#constructor","title":"Constructor","text":""},{"location":"api/pandas/#__init__df-dataframe-none","title":"<code>__init__(df: DataFrame) -&gt; None</code>","text":"<p>Creates a new Pandas data adapter from a DataFrame.</p> <p>Arguments:</p> <ul> <li><code>df</code>: Pandas DataFrame containing the data</li> </ul> <p>Validation:</p> <ul> <li>Minimum 2 rows and 2 columns required</li> <li>No missing data (NaN values) allowed</li> <li>All column names must be strings</li> </ul> <p>Raises:</p> <ul> <li><code>TypeError</code>: If df is not a pandas DataFrame</li> <li><code>ValueError</code>: If DataFrame size or data validation fails</li> </ul>"},{"location":"api/pandas/#class-methods","title":"Class Methods","text":""},{"location":"api/pandas/#readfilename-dstype-nnone-kwargs-pandas","title":"<code>read(filename, dstype, N=None, **kwargs) -&gt; 'Pandas'</code>","text":"<p>Factory method to create a Pandas instance by reading data from a file.</p> <p>Arguments:</p> <ul> <li><code>filename</code>: Path to data file (supports .csv, .gz compression)</li> <li><code>dstype</code>: Dataset type (\"categorical\", \"continuous\", or \"mixed\")  </li> <li><code>N</code>: Optional sample size limit</li> <li><code>**kwargs</code>: Additional arguments for pandas.read_csv()</li> </ul> <p>Features: - Automatic compression detection for .gz files - Intelligent type inference and conversion - Categorical variable encoding - Memory-efficient loading for large files</p> <p>Example: <pre><code># Load categorical data\ndata = Pandas.read(\"dataset.csv\", dstype=\"categorical\")\n\n# Load with custom separator and sample size\ndata = Pandas.read(\"data.tsv\", dstype=\"mixed\", N=10000, sep='\\t')\n</code></pre></p>"},{"location":"api/pandas/#data-management","title":"Data Management","text":""},{"location":"api/pandas/#set_nn-seednone-random_selectionfalse-none","title":"<code>set_N(N, seed=None, random_selection=False) -&gt; None</code>","text":"<p>Sets the working sample size with optional randomization.</p> <p>Arguments:</p> <ul> <li><code>N</code>: Target sample size (must be \u2264 original data size)</li> <li><code>seed</code>: Randomization seed for reproducible sampling</li> <li><code>random_selection</code>: If True, randomly selects rows; if False, uses first N rows</li> </ul> <p>Behavior:</p> <ul> <li>Updates internal <code>_sample</code> DataFrame with the specified subset</li> <li>Preserves original data in <code>df</code> attribute</li> <li>Recomputes categorical value counts for the new sample</li> </ul>"},{"location":"api/pandas/#randomise_namesseednone-none","title":"<code>randomise_names(seed=None) -&gt; None</code>","text":"<p>Randomizes node names for algorithm sensitivity testing.</p> <p>Arguments:</p> <ul> <li><code>seed</code>: Randomization seed, or None to revert to original names</li> </ul> <p>Implementation:</p> <ul> <li>Generates randomized column names using format <code>X###NNNNNN</code></li> <li>Updates DataFrame column names in-place</li> <li>Maintains mappings between original and external names</li> <li>Updates sample DataFrame to reflect name changes</li> </ul>"},{"location":"api/pandas/#statistical-operations","title":"Statistical Operations","text":""},{"location":"api/pandas/#marginalsnode-parents-values_reqdfalse-tuple","title":"<code>marginals(node, parents, values_reqd=False) -&gt; Tuple</code>","text":"<p>Computes marginal distributions for a node given its parents.</p> <p>Arguments:</p> <ul> <li><code>node</code>: Target node name (external name)</li> <li><code>parents</code>: Dictionary of parent values <code>{parent_name: value}</code></li> <li><code>values_reqd</code>: If True, returns actual values; if False, returns counts only</li> </ul> <p>Returns:</p> <ul> <li>Tuple of (marginal_counts, unique_values) for categorical data</li> <li>For continuous data, returns appropriate statistical summaries</li> </ul> <p>Implementation:</p> <ul> <li>Uses pandas crosstab for efficient categorical marginalization</li> <li>Handles continuous variables with binning strategies</li> <li>Optimized for sparse parent configurations</li> </ul>"},{"location":"api/pandas/#valuesnodes-tuplestr-npndarray","title":"<code>values(nodes: Tuple[str, ...]) -&gt; np.ndarray</code>","text":"<p>Returns the actual data values for specified nodes.</p> <p>Arguments:</p> <ul> <li><code>nodes</code>: Tuple of node names (external names)</li> </ul> <p>Returns:</p> <ul> <li>NumPy array with shape (N, len(nodes)) containing the data values</li> </ul> <p>Usage: <pre><code># Get values for specific variables\nsubset = data.values((\"var1\", \"var2\", \"var3\"))\nprint(subset.shape)  # (N, 3)\n</code></pre></p>"},{"location":"api/pandas/#properties","title":"Properties","text":""},{"location":"api/pandas/#node-information","title":"Node Information","text":"<ul> <li><code>nodes</code>: Original column names from DataFrame</li> <li><code>node_types</code>: Mapping of node names to their data types</li> <li><code>node_values</code>: Value counts for categorical variables only</li> </ul>"},{"location":"api/pandas/#sample-access","title":"Sample Access","text":"<ul> <li><code>N</code>: Current working sample size</li> <li><code>sample</code>: Current working sample as DataFrame</li> </ul>"},{"location":"api/pandas/#data-conversion","title":"Data Conversion","text":""},{"location":"api/pandas/#as_df-dataframe","title":"<code>as_df() -&gt; DataFrame</code>","text":"<p>Returns the current working sample as a pandas DataFrame.</p> <p>Returns:</p> <ul> <li>DataFrame with external column names and current sample data</li> </ul> <p>Usage: <pre><code># Access current sample\ncurrent_df = data.as_df()\n\n# Convert to NumPy for performance-critical operations\nnumpy_data = NumPy.from_df(current_df, dstype=data.dstype)\n</code></pre></p>"},{"location":"api/pandas/#file-io","title":"File I/O","text":""},{"location":"api/pandas/#writefilename-compressfalse-sf10-zeronone-preservetrue-none","title":"<code>write(filename, compress=False, sf=10, zero=None, preserve=True) -&gt; None</code>","text":"<p>Writes the current sample to a CSV file.</p> <p>Arguments:</p> <ul> <li><code>filename</code>: Output file path</li> <li><code>compress</code>: Whether to gzip compress the output</li> <li><code>sf</code>: Significant figures for floating-point data</li> <li><code>zero</code>: Value to replace zeros with (for numerical stability)</li> <li><code>preserve</code>: Whether to preserve original formatting</li> </ul> <p>Features:</p> <ul> <li>Automatic compression if filename ends with .gz</li> <li>Configurable precision for floating-point output</li> <li>Handles categorical data appropriately</li> <li>Preserves data integrity during round-trip operations</li> </ul>"},{"location":"api/pandas/#type-handling","title":"Type Handling","text":""},{"location":"api/pandas/#automatic-type-inference","title":"Automatic Type Inference","text":"<p>The Pandas adapter automatically infers and converts data types:</p> <p>Categorical Data:</p> <ul> <li>String columns are converted to pandas categorical type</li> <li>Integer columns with limited unique values become categorical</li> <li>Maintains category ordering where applicable</li> </ul> <p>Numeric Data:</p> <ul> <li>Floating-point columns preserve precision</li> <li>Integer columns use appropriate NumPy integer types</li> <li>Mixed columns are handled according to dstype parameter</li> </ul>"},{"location":"api/pandas/#type-validation","title":"Type Validation","text":"<pre><code># Dataset type is automatically determined\nif data.dstype == \"categorical\":\n    print(\"All variables are categorical\")\nelif data.dstype == \"continuous\":\n    print(\"All variables are numeric\")  \nelse:  # \"mixed\"\n    print(\"Mixed variable types detected\")\n</code></pre>"},{"location":"api/pandas/#memory-management","title":"Memory Management","text":""},{"location":"api/pandas/#efficient-sampling","title":"Efficient Sampling","text":"<ul> <li>Original DataFrame is preserved in <code>df</code> attribute</li> <li>Working sample stored separately in <code>_sample</code></li> <li>View-based operations where possible to minimize copying</li> </ul>"},{"location":"api/pandas/#lazy-operations","title":"Lazy Operations","text":"<ul> <li>Type conversions performed only when necessary</li> <li>Value counts computed on-demand for categorical variables</li> <li>Sample updates triggered only when needed</li> </ul>"},{"location":"api/pandas/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"api/pandas/#best-use-cases","title":"Best Use Cases","text":"<ul> <li>Exploratory data analysis and prototyping</li> <li>Moderate-sized datasets (&lt; 100K rows typically)</li> <li>Mixed data types requiring pandas functionality</li> <li>File I/O with various formats and options</li> </ul>"},{"location":"api/pandas/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Memory overhead due to pandas metadata</li> <li>String operations can be slow for large categorical data</li> <li>DataFrame operations generally slower than pure NumPy</li> </ul>"},{"location":"api/pandas/#integration-examples","title":"Integration Examples","text":""},{"location":"api/pandas/#with-numpy-adapter","title":"With NumPy Adapter","text":"<pre><code># Load and explore with Pandas\npandas_data = Pandas.read(\"data.csv\", dstype=\"mixed\")\nprint(pandas_data.as_df().describe())\n\n# Convert to NumPy for computational efficiency\nnumpy_data = NumPy.from_df(pandas_data.as_df(), \n                          dstype=pandas_data.dstype,\n                          keep_df=True)\n</code></pre>"},{"location":"api/pandas/#workflow-integration","title":"Workflow Integration","text":"<pre><code># Load data\ndata = Pandas.read(\"experiment.csv\", dstype=\"categorical\")\n\n# Set up experiment parameters\ndata.set_N(5000, seed=42, random_selection=True)\ndata.randomise_names(seed=123)\n\n# Run causal discovery algorithm\nresults = discovery_algorithm.run(data)\n\n# Save results\ndata.write(\"experiment_sample.csv\", compress=True)\n</code></pre>"},{"location":"api/preprocess/","title":"Preprocess - Data Preprocessing Utilities","text":"<p>The <code>preprocess</code> module provides utilities for cleaning and preparing data for Bayesian Network operations and causal discovery workflows. These functions help address common data quality issues that can interfere with structure learning and parameter estimation.</p>"},{"location":"api/preprocess/#overview","title":"Overview","text":"<p>Data preprocessing is often a critical step before applying causal discovery algorithms or fitting Bayesian Networks. Real-world datasets and even synthetic data can contain variables that provide no useful information or can cause algorithmic issues. The preprocess module provides utilities to identify and remove such problematic variables.</p>"},{"location":"api/preprocess/#functions","title":"Functions","text":""},{"location":"api/preprocess/#remove_single_valuedbn-data-tuplebn-dataframe-liststr","title":"<code>remove_single_valued(bn, data) -&gt; Tuple[BN, DataFrame, List[str]]</code>","text":"<p>Removes nodes from a Bayesian Network that contain only a single unique value in the corresponding data. This is particularly useful when working with synthetic data or datasets where some variables have become constant due to filtering or other preprocessing steps.</p> <p>Arguments:</p> <ul> <li><code>bn</code>: Bayesian Network object to modify (BN)</li> <li><code>data</code>: Pandas DataFrame containing the data corresponding to the BN</li> </ul> <p>Returns:</p> <p>Tuple containing: - Modified BN: New Bayesian Network with single-valued variables removed - Cleaned DataFrame: Data with single-valued columns removed and categorical types applied - Removed Variables: List of variable names that were removed (sorted)</p> <p>Raises:</p> <ul> <li><code>TypeError</code>: If data is not a pandas DataFrame</li> <li><code>ValueError</code>: If removing single-valued variables would leave fewer than 2 multi-valued variables</li> </ul> <p>Functionality:</p> <ol> <li>Variable Analysis: Identifies columns in the data that have fewer than 2 unique values</li> <li>Network Restructuring: Creates a new BN with problematic nodes removed from:</li> <li>Node list </li> <li>Edge relationships (removes edges involving removed nodes)</li> <li>Network structure</li> <li>Data Cleaning: Removes corresponding columns from the DataFrame and ensures categorical typing</li> <li>Validation: Ensures at least 2 variables remain after cleaning</li> </ol> <p>Usage Examples:</p> <pre><code>from causaliq_data.preprocess import remove_single_valued\nfrom causaliq_core.bn.io import read_bn\nimport pandas as pd\n\n# Load a Bayesian Network and some data\noriginal_bn = read_bn(\"network.dsc\")\ndata = pd.read_csv(\"dataset.csv\")\n\n# Remove single-valued variables\ncleaned_bn, cleaned_data, removed_vars = remove_single_valued(original_bn, data)\n\nprint(f\"Original variables: {len(original_bn.nodes)}\")\nprint(f\"Cleaned variables: {len(cleaned_bn.nodes)}\")\nprint(f\"Removed variables: {removed_vars}\")\n\n# Use the cleaned network and data for further analysis\n# ... proceed with structure learning or parameter estimation\n</code></pre> <p>Common Use Cases:</p>"},{"location":"api/preprocess/#synthetic-data-cleaning","title":"Synthetic Data Cleaning","text":"<pre><code># When generating synthetic data, some variables might accidentally become constant\nfrom causaliq_data.oracle import Oracle\n\n# Generate synthetic data from a BN\noracle = Oracle(bn=network, N=1000)\nsynthetic_data = oracle.sample\n\n# Check for and remove any single-valued variables that might have emerged\nif any(synthetic_data.nunique() &lt; 2):\n    cleaned_bn, cleaned_data, removed = remove_single_valued(network, synthetic_data)\n    print(f\"Removed constant variables: {removed}\")\n</code></pre>"},{"location":"api/preprocess/#real-data-preprocessing","title":"Real Data Preprocessing","text":"<pre><code># Real datasets often have variables that become single-valued after filtering\noriginal_data = pd.read_csv(\"survey_data.csv\")\n\n# After applying filters, some variables might become constant\nfiltered_data = original_data[original_data['age'] &gt; 65]  # Example filter\n\n# Remove any variables that became single-valued after filtering\nif any(filtered_data.nunique() &lt; 2):\n    # Fit initial BN from filtered data structure\n    from causaliq_data.pandas import Pandas\n    data_adapter = Pandas(filtered_data)\n    initial_bn = BN.fit(complete_graph(filtered_data.columns), data_adapter)\n\n    # Clean up single-valued variables\n    final_bn, final_data, removed = remove_single_valued(initial_bn, filtered_data)\n</code></pre> <p>Technical Details:</p>"},{"location":"api/preprocess/#variable-detection","title":"Variable Detection","text":"<p>The function uses pandas' <code>nunique()</code> method to count unique values per column: <pre><code>single_valued = [col for col, count in data.nunique().items() if count &lt; 2]\n</code></pre></p> <p>This identifies variables that have: - Exactly 1 unique value (constant variables) - 0 unique values (empty variables, though rare)</p>"},{"location":"api/preprocess/#network-reconstruction","title":"Network Reconstruction","text":"<p>The cleaned Bayesian Network is reconstructed by:</p> <ol> <li>Node Filtering: Removing problematic nodes from the node list</li> <li>Edge Filtering: Keeping only edges where both source and target nodes remain</li> <li>Structure Preservation: Maintaining all valid conditional dependencies</li> <li>Parameter Re-estimation: Fitting parameters using the cleaned data</li> </ol>"},{"location":"api/preprocess/#data-type-management","title":"Data Type Management","text":"<p>The function ensures proper categorical typing on the cleaned data, which is important for: - Consistent handling in downstream BN operations - Memory efficiency with categorical variables - Compatibility with causaliq-data adapters</p>"},{"location":"api/preprocess/#error-handling","title":"Error Handling","text":"<p>The function provides comprehensive validation:</p> <p>Type Validation: <pre><code>if not isinstance(data, DataFrame):\n    raise TypeError(\"remove_single_valued_variables() bad arg type\")\n</code></pre></p> <p>Minimum Variable Requirements: <pre><code>if len(data.columns) - len(remove) &lt; 2:\n    raise ValueError(\"remove_single_valued_variables() - &lt;2 multi-valued\")\n</code></pre></p> <p>This ensures that the resulting dataset has enough variables for meaningful analysis, as Bayesian Networks require at least two variables to represent relationships.</p>"},{"location":"api/preprocess/#integration-with-causaliq-workflow","title":"Integration with CausalIQ Workflow","text":"<p>The preprocess module integrates seamlessly with other causaliq-data components:</p>"},{"location":"api/preprocess/#with-data-adapters","title":"With Data Adapters","text":"<pre><code>from causaliq_data.pandas import Pandas\nfrom causaliq_data.preprocess import remove_single_valued\n\n# Start with raw data\nraw_data = pd.read_csv(\"dataset.csv\")\ndata_adapter = Pandas(raw_data)\n\n# If needed, clean up single-valued variables\nif any(raw_data.nunique() &lt; 2):\n    # Create initial BN structure (e.g., complete graph for testing)\n    initial_bn = BN.fit(complete_graph(raw_data.columns), data_adapter)\n    cleaned_bn, cleaned_data, removed = remove_single_valued(initial_bn, raw_data)\n\n    # Create new adapter with cleaned data\n    final_adapter = Pandas(cleaned_data)\n</code></pre>"},{"location":"api/preprocess/#with-structure-learning","title":"With Structure Learning","text":"<pre><code>from causaliq_discovery import pc_algorithm  # Example\n\n# Preprocess data before structure learning\ncleaned_bn, cleaned_data, removed_vars = remove_single_valued(initial_bn, raw_data)\n\n# Apply structure learning on cleaned data\nlearned_structure = pc_algorithm(cleaned_data)\n</code></pre>"},{"location":"api/preprocess/#with-independence-testing","title":"With Independence Testing","text":"<pre><code>from causaliq_data.indep import indep\n\n# Clean data first to avoid issues with constant variables\ncleaned_bn, cleaned_data, removed = remove_single_valued(bn, data)\n\n# Now independence tests will work properly (no zero-variance variables)\ntest_result = indep(\"X\", \"Y\", [\"Z\"], cleaned_data)\n</code></pre>"},{"location":"api/preprocess/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Memory Efficiency: Creates new objects rather than modifying in-place to preserve original data</li> <li>Categorical Optimization: Applies categorical typing for memory efficiency</li> <li>Network Reconstruction: Uses efficient BN.fit() method for parameter estimation</li> <li>Minimal Overhead: Only processes data if single-valued variables are detected</li> </ul>"},{"location":"api/preprocess/#best-practices","title":"Best Practices","text":"<ol> <li>Early Preprocessing: Apply before structure learning to avoid algorithmic issues</li> <li>Validation: Always check the removed variable list to ensure expected variables weren't accidentally removed</li> <li>Documentation: Record which variables were removed for reproducibility</li> <li>Threshold Consideration: The function removes variables with &lt;2 unique values, which is appropriate for discrete/categorical data</li> </ol>"},{"location":"api/preprocess/#see-also","title":"See Also","text":"<ul> <li>Data: Base data handling interface</li> <li>Pandas: DataFrame-based data adapter</li> <li>Independence Testing: Statistical independence tests</li> <li>CausalIQ Core: Bayesian Network and DAG functionality</li> </ul>"},{"location":"api/score/","title":"Score - Scoring Functions for Causal Structure Learning","text":"<p>The <code>score</code> module provides comprehensive scoring functions for evaluating Bayesian networks and DAGs. It implements various entropy-based and Bayesian scoring methods for both categorical and continuous (Gaussian) data.</p>"},{"location":"api/score/#score-types","title":"Score Types","text":""},{"location":"api/score/#categorical-data-scores","title":"Categorical Data Scores","text":""},{"location":"api/score/#entropy-based-scores","title":"Entropy-Based Scores","text":"<ul> <li><code>loglik</code>: Log-likelihood score</li> <li><code>bic</code>: Bayesian Information Criterion</li> <li><code>aic</code>: Akaike Information Criterion</li> </ul>"},{"location":"api/score/#bayesian-scores","title":"Bayesian Scores","text":"<ul> <li><code>bde</code>: Bayesian Dirichlet Equivalent (with hyperparameters)</li> <li><code>k2</code>: K2 score</li> <li><code>bdj</code>: Bayesian Dirichlet with Jeffreys prior</li> <li><code>bds</code>: Bayesian Dirichlet Sparse</li> </ul>"},{"location":"api/score/#gaussian-data-scores","title":"Gaussian Data Scores","text":"<ul> <li><code>bic-g</code>: Gaussian BIC</li> <li><code>bge</code>: Bayesian Gaussian Equivalent</li> <li><code>loglik-g</code>: Gaussian log-likelihood</li> </ul>"},{"location":"api/score/#constants","title":"Constants","text":""},{"location":"api/score/#available-scores","title":"Available Scores","text":"<pre><code>ENTROPY_SCORES = [\"loglik\", \"bic\", \"aic\"]\nBAYESIAN_SCORES = [\"bde\", \"k2\", \"bdj\", \"bds\"] \nGAUSSIAN_SCORES = [\"bic-g\", \"bge\", \"loglik-g\"]\n</code></pre>"},{"location":"api/score/#score-parameters","title":"Score Parameters","text":"<pre><code>SCORES = {\n    \"loglik\": {\"base\"},\n    \"loglik-g\": {\"base\"}, \n    \"aic\": {\"base\", \"k\"},\n    \"bic\": {\"base\", \"k\"},\n    \"bic-g\": {\"base\", \"k\"},\n    \"bge\": {},\n    \"bde\": {\"iss\", \"prior\"},\n    \"bds\": {\"iss\", \"prior\"},\n    \"bdj\": {},\n    \"k2\": {},\n}\n\nSCORE_PARAMS = {\n    \"base\": \"e\",\n    \"k\": 1,\n    \"iss\": 1, \n    \"prior\": \"uniform\",\n    \"unistate_ok\": True,\n}\n</code></pre>"},{"location":"api/score/#core-functions","title":"Core Functions","text":""},{"location":"api/score/#node-scoring","title":"Node Scoring","text":""},{"location":"api/score/#node_scorenode-parents-types-params-data-dictstr-float","title":"<code>node_score(node, parents, types, params, data) -&gt; Dict[str, float]</code>","text":"<p>Computes specified score types for a single node given its parents.</p> <p>Arguments:</p> <ul> <li><code>node</code>: Node name to score</li> <li><code>parents</code>: Dictionary mapping nodes to their parent lists</li> <li><code>types</code>: Score type(s) to compute (string or list)</li> <li><code>params</code>: Score parameters dictionary</li> <li><code>data</code>: Data object implementing the Data interface</li> </ul> <p>Returns:</p> <ul> <li>Dictionary mapping score types to computed values</li> </ul>"},{"location":"api/score/#categorical_node_scorenode-parents-types-params-data-counts_reqdfalse","title":"<code>categorical_node_score(node, parents, types, params, data, counts_reqd=False)</code>","text":"<p>Returns decomposable scores for a categorical node with specified parents.</p> <p>Arguments:</p> <ul> <li><code>node</code>: Node name to score</li> <li><code>parents</code>: Parents dictionary</li> <li><code>types</code>: List of score types required</li> <li><code>params</code>: Score parameters</li> <li><code>data</code>: Data object</li> <li><code>counts_reqd</code>: Whether to return marginal count information</li> </ul> <p>Returns:</p> <ul> <li>Dictionary of scores, or tuple of (scores, counts_info) if counts_reqd=True</li> </ul>"},{"location":"api/score/#gaussian_node_scorenode-parents-types-params-data-dictstr-float","title":"<code>gaussian_node_score(node, parents, types, params, data) -&gt; Dict[str, float]</code>","text":"<p>Computes Gaussian scores for continuous nodes.</p> <p>Arguments:</p> <ul> <li><code>node</code>: Node name to score</li> <li><code>parents</code>: Parents dictionary  </li> <li><code>types</code>: List of Gaussian score types</li> <li><code>params</code>: Score parameters</li> <li><code>data</code>: Data object</li> </ul> <p>Returns:</p> <ul> <li>Dictionary mapping score types to values</li> </ul>"},{"location":"api/score/#dag-and-network-scoring","title":"DAG and Network Scoring","text":""},{"location":"api/score/#dag_scoredag-data-types-params-dataframe","title":"<code>dag_score(dag, data, types, params) -&gt; DataFrame</code>","text":"<p>Returns per-node scores for a complete DAG given data.</p> <p>Arguments:</p> <ul> <li><code>dag</code>: DAG object to score</li> <li><code>data</code>: Data object (not Oracle type)</li> <li><code>types</code>: Score type(s) required (string or list)</li> <li><code>params</code>: Score parameters dictionary</li> </ul> <p>Returns:</p> <ul> <li>DataFrame with nodes as rows and score types as columns</li> </ul> <p>Raises:</p> <ul> <li><code>TypeError</code>: For invalid argument types</li> <li><code>ValueError</code>: For invalid argument values or data/DAG mismatch</li> </ul>"},{"location":"api/score/#bn_scorebn-n-types-params-dataframe","title":"<code>bn_score(bn, N, types, params) -&gt; DataFrame</code>","text":"<p>Returns per-node scores for a Bayesian Network using oracle scoring.</p> <p>Arguments:</p> <ul> <li><code>bn</code>: BN object to score</li> <li><code>N</code>: Dataset size to assume</li> <li><code>types</code>: Score type(s) required </li> <li><code>params</code>: Score parameters dictionary</li> </ul> <p>Returns:</p> <ul> <li>DataFrame with nodes as rows and score types as columns</li> </ul> <p>Raises:</p> <ul> <li><code>TypeError</code>: For invalid argument types</li> <li><code>ValueError</code>: For invalid score types or non-positive N</li> </ul>"},{"location":"api/score/#specialized-score-functions","title":"Specialized Score Functions","text":""},{"location":"api/score/#bayesian_scoren_ijk-q_i-type-params-float","title":"<code>bayesian_score(N_ijk, q_i, type, params) -&gt; float</code>","text":"<p>Computes Bayesian-based scores for marginal counts of a single node.</p> <p>Arguments:</p> <ul> <li><code>N_ijk</code>: 2D array of instance counts for node i, parental combo j, and node value k</li> <li><code>q_i</code>: Maximum possible number of parental value combinations</li> <li><code>type</code>: Bayesian score type (bde, k2, bdj, bds)</li> <li><code>params</code>: Parameters dictionary including 'iss' (imaginary sample size)</li> </ul> <p>Returns:</p> <ul> <li>Computed Bayesian score value</li> </ul>"},{"location":"api/score/#entropy_scoresnijk-types-params-n-free_params-dictstr-float","title":"<code>entropy_scores(Nijk, types, params, N, free_params) -&gt; Dict[str, float]</code>","text":"<p>Returns entropy-based scores for marginal counts.</p> <p>Arguments:</p> <ul> <li><code>Nijk</code>: 2D array of instance counts</li> <li><code>types</code>: Entropy score types required</li> <li><code>params</code>: Parameters including logarithm 'base'</li> <li><code>N</code>: Number of cases (instances)  </li> <li><code>free_params</code>: Number of free parameters</li> </ul> <p>Returns:</p> <ul> <li>Dictionary of requested entropy scores</li> </ul>"},{"location":"api/score/#bayesian_gaussian_scorenode-parents-params-data-float","title":"<code>bayesian_gaussian_score(node, parents, params, data) -&gt; float</code>","text":"<p>Computes Bayesian Gaussian Equivalent (BGE) score for a node.</p> <p>Arguments:</p> <ul> <li><code>node</code>: Node to score</li> <li><code>parents</code>: Parents dictionary</li> <li><code>params</code>: Score parameters</li> <li><code>data</code>: Data object</li> </ul> <p>Returns:</p> <ul> <li>BGE score value</li> </ul>"},{"location":"api/score/#entropy_gaussian_scorenode-parents-params-data-dictstr-float","title":"<code>entropy_gaussian_score(node, parents, params, data) -&gt; Dict[str, float]</code>","text":"<p>Returns entropy-based scores for Gaussian nodes.</p> <p>Arguments:</p> <ul> <li><code>node</code>: Node to score</li> <li><code>parents</code>: Parents dictionary</li> <li><code>params</code>: Score parameters  </li> <li><code>data</code>: Data object</li> </ul> <p>Returns:</p> <ul> <li>Dictionary with 'bic-g' and 'loglik-g' scores</li> </ul>"},{"location":"api/score/#utility-functions","title":"Utility Functions","text":""},{"location":"api/score/#check_score_paramsparams-scoresnone-dictstr-any","title":"<code>check_score_params(params, scores=None) -&gt; Dict[str, Any]</code>","text":"<p>Validates and completes score parameters with defaults.</p> <p>Arguments:</p> <ul> <li><code>params</code>: Parameters dictionary to validate</li> <li><code>scores</code>: Optional list of score types to validate against</li> </ul> <p>Returns:</p> <ul> <li>Validated and completed parameters dictionary</li> </ul> <p>Raises:</p> <ul> <li><code>TypeError</code>: For invalid parameter types</li> <li><code>ValueError</code>: For invalid parameter values</li> </ul>"},{"location":"api/score/#free_paramsgraph-data-debugfalse-int","title":"<code>free_params(graph, data, debug=False) -&gt; int</code>","text":"<p>Calculates the total number of free parameters in a graph structure.</p> <p>Arguments:</p> <ul> <li><code>graph</code>: Graph object (DAG or PDAG)</li> <li><code>data</code>: Data object  </li> <li><code>debug</code>: Whether to print debug information</li> </ul> <p>Returns:</p> <ul> <li>Total number of free parameters</li> </ul>"},{"location":"api/score/#usage-examples","title":"Usage Examples","text":""},{"location":"api/score/#basic-node-scoring","title":"Basic Node Scoring","text":"<pre><code>from causaliq_data.score import node_score\n\n# Score a node with categorical data\nscores = node_score(\n    node=\"X1\", \n    parents={\"X1\": [\"X0\", \"X2\"]},\n    types=[\"bic\", \"bde\"],\n    params={\"base\": \"e\", \"iss\": 1},\n    data=data_obj\n)\n</code></pre>"},{"location":"api/score/#dag-scoring","title":"DAG Scoring","text":"<pre><code>from causaliq_data.score import dag_score\n\n# Score entire DAG\ndf_scores = dag_score(\n    dag=my_dag,\n    data=my_data, \n    types=[\"bic\", \"loglik\"],\n    params={\"base\": \"e\"}\n)\n</code></pre>"},{"location":"api/score/#gaussian-scoring","title":"Gaussian Scoring","text":"<pre><code>from causaliq_data.score import gaussian_node_score\n\n# Score continuous node\ngaussian_scores = gaussian_node_score(\n    node=\"Y1\",\n    parents={\"Y1\": [\"Y0\"]}, \n    types=[\"bic-g\", \"bge\"],\n    params={\"base\": \"e\"},\n    data=continuous_data\n)\n</code></pre>"},{"location":"api/score/#notes","title":"Notes","text":"<ul> <li>Score functions automatically determine appropriate score types based on data type</li> <li>Parameters are validated and defaults applied via <code>check_score_params()</code> </li> <li>BGE implementation follows bnlearn defaults with some simplifications</li> <li>Entropy scores require sufficient sample sizes for reliable estimates</li> <li>Single-valued variables will raise errors unless <code>unistate_ok=True</code></li> </ul>"},{"location":"architecture/bnfit_interface_spec/","title":"CausalIQ Data Interface Specification","text":""},{"location":"architecture/bnfit_interface_spec/#overview","title":"Overview","text":"<p>This document defines the interface contract between CausalIQ Core and CausalIQ Data packages. The core package contains BN fitting algorithms (CPT.fit, LinGauss.fit) that require data access operations. The data package will provide concrete implementations of data sources.</p>"},{"location":"architecture/bnfit_interface_spec/#abstract-data-interface","title":"Abstract Data Interface","text":""},{"location":"architecture/bnfit_interface_spec/#bnfit-abstract-base-class","title":"<code>BNFit</code> (Abstract Base Class)","text":"<p>The core package requires data sources to implement this interface:</p> <pre><code>from abc import ABC, abstractmethod\nfrom typing import Dict, Tuple, Any\nimport numpy as np\n\nclass BNFit(ABC):\n    \"\"\"Abstract interface for data sources used in BN fitting.\"\"\"\n\n    @abstractmethod\n    def marginals(self, node: str, parents: Dict, values_reqd: bool = False) -&gt; Tuple:\n        \"\"\"Return marginal counts for a node and its parents.\n\n        Args:\n            node (str): Node for which marginals required.\n            parents (dict): {node: parents} parents of non-orphan nodes\n            values_reqd (bool): Whether parent and child values required\n\n        Returns:\n            tuple: Of counts, and optionally, values:\n                   - ndarray counts: 2D, rows=child, cols=parents\n                   - int maxcol: Maximum number of parental values\n                   - tuple rowval: Child values for each row\n                   - tuple colval: Parent combo (dict) for each col\n\n        Raises:\n            TypeError: For bad argument types\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def values(self, columns: Tuple[str, ...]) -&gt; np.ndarray:\n        \"\"\"Return the (float) values for the specified set of columns.\n\n        Suitable for passing into e.g. linearRegression fitting function\n\n        Args:\n            columns (tuple): Columns for which data required\n\n        Returns:\n            ndarray: Numpy array of values, each column for a node\n\n        Raises:\n            TypeError: If bad arg type\n            ValueError: If bad arg value\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def N(self) -&gt; int:\n        \"\"\"Total sample size.\n\n        Returns:\n            int: Current sample size being used\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def node_values(self) -&gt; Dict[str, Dict]:\n        \"\"\"Node value counts for categorical variables.\n\n        Returns:\n            dict: Values and their counts of categorical nodes\n                  in sample {n1: {v1: c1, v2: ...}, n2 ...}\n        \"\"\"\n        pass\n</code></pre>"},{"location":"architecture/bnfit_interface_spec/#usage-in-core-package","title":"Usage in Core Package","text":""},{"location":"architecture/bnfit_interface_spec/#cptfit-dependencies","title":"CPT.fit() Dependencies","text":"<p>The CPT.fit() method requires these data operations:</p> <pre><code># For nodes with parents\ncounts, _, rowval, colval = data.marginals(node, {node: list(parents)}, True)\n\n# For autocomplete functionality\ndata.N  # Total sample size\ndata.node_values[node]  # {value: count} for node\ndata.node_values[parent]  # {value: count} for each parent\n\n# For orphan nodes\ndata.N\ndata.node_values[node]\n</code></pre>"},{"location":"architecture/bnfit_interface_spec/#lingaussfit-dependencies","title":"LinGauss.fit() Dependencies","text":"<p>The LinGauss.fit() method requires:</p> <pre><code># Get continuous values for regression\nvalues = data.values((node,))  # For orphan nodes\nvalues = data.values(tuple([node] + list(parents)))  # For nodes with parents\n</code></pre>"},{"location":"architecture/bnfit_interface_spec/#expected-concrete-implementations","title":"Expected Concrete Implementations","text":"<p>The data package should provide these concrete classes:</p>"},{"location":"architecture/bnfit_interface_spec/#1-legacypandasadapter","title":"1. <code>LegacyPandasAdapter</code>","text":"<ul> <li>Adapts existing legacy.data.Pandas class to DataInterface</li> <li>Delegates to existing marginals(), values(), N, node_values implementations</li> <li>Handles pandas DataFrames efficiently with crosstab-based marginals</li> <li>Ensures backward compatibility with existing test suites</li> </ul>"},{"location":"architecture/bnfit_interface_spec/#2-legacynumpyadapter","title":"2. <code>LegacyNumPyAdapter</code>","text":"<ul> <li>Adapts existing legacy.data.NumPy class to BNFit interface  </li> <li>Delegates to existing marginals(), values(), N, node_values implementations</li> <li>Enables NumPy support that doesn't currently work with core algorithms</li> <li>For NumPy-based data sources</li> </ul>"},{"location":"architecture/bnfit_interface_spec/#future-extensions","title":"Future Extensions","text":"<p>The interface is designed to be extensible for: - GPU-accelerated data sources (e.g., CuPy, Rapids cuDF) - Database backends (SQL, NoSQL) - Streaming data sources  - Distributed data processing (Dask, Spark) - Custom data transformations</p>"},{"location":"architecture/bnfit_interface_spec/#legacy-compatibility-requirements","title":"Legacy Compatibility Requirements","text":"<p>The data package must maintain compatibility with existing usage patterns:</p> <pre><code># Existing legacy pattern that must continue working\nfrom legacy.data.pandas import Pandas\ndata = Pandas(df)\ncnd_spec, estimated = CPT.fit('B', ('A',), data)\n\n# New pattern with adapter\nfrom causaliq_data import LegacyDataAdapter\ndata_adapted = LegacyDataAdapter(data)\ncnd_spec, estimated = CPT.fit('B', ('A',), data_adapted)\n</code></pre>"},{"location":"architecture/bnfit_interface_spec/#key-implementation-details","title":"Key Implementation Details","text":""},{"location":"architecture/bnfit_interface_spec/#marginals-method-behavior","title":"marginals() Method Behavior","text":"<p>For orphan nodes (no parents): - <code>parents</code> parameter: <code>{}</code> or <code>{node: []}</code> - Returns: <code>(counts.reshape(-1, 1), 1, rowval, colval)</code> - <code>rowval</code>: tuple of node values - <code>colval</code>: tuple containing single empty dict <code>({},)</code></p> <p>For nodes with single parent: - <code>parents</code> parameter: <code>{node: [parent_name]}</code> - Returns: <code>(counts_2d, num_cols, rowval, colval)</code> - <code>rowval</code>: tuple of child values - <code>colval</code>: tuple of dicts <code>({parent: value},)</code></p> <p>For nodes with multiple parents: - <code>parents</code> parameter: <code>{node: [parent1, parent2, ...]}</code> - Returns: <code>(counts_2d, num_cols, rowval, colval)</code> - <code>colval</code>: tuple of dicts <code>({parent1: val1, parent2: val2},)</code></p>"},{"location":"architecture/bnfit_interface_spec/#values-method-behavior","title":"values() Method Behavior","text":"<ul> <li>Must return numpy array with float dtype</li> <li>Each column corresponds to a requested node</li> <li>Row order must be consistent with data source</li> <li>Should validate that all requested columns exist</li> </ul>"},{"location":"architecture/bnfit_interface_spec/#error-handling-standards","title":"Error Handling Standards","text":"<p>All methods should raise: - <code>TypeError</code>: For incorrect argument types - <code>ValueError</code>: For invalid argument values (missing columns, etc.)</p>"},{"location":"architecture/bnfit_interface_spec/#migration-path","title":"Migration Path","text":"<ol> <li>Phase 1: Create data package with interface and implementations</li> <li>Phase 2: Update core package to import DataInterface from data package</li> <li>Phase 3: Update legacy tests to use adapters</li> <li>Phase 4: Add new data source types as needed</li> </ol>"},{"location":"architecture/bnfit_interface_spec/#testing-requirements","title":"Testing Requirements","text":"<p>The data package should include: - Unit tests for each concrete implementation - Integration tests with core CPT.fit() and LinGauss.fit() - Compatibility tests with legacy test suite - Performance benchmarks for marginals calculation</p>"},{"location":"architecture/bnfit_interface_spec/#future-extensions_1","title":"Future Extensions","text":"<p>The interface is designed to be extensible for: - Database backends - Streaming data sources - Distributed data processing - Custom data transformations</p>"},{"location":"architecture/bnfit_interface_spec/#notes-for-implementation","title":"Notes for Implementation","text":"<ul> <li>Prioritize performance in marginals() calculation (this is the bottleneck)</li> <li>Consider caching computed marginals for repeated queries</li> <li>Ensure thread safety if needed for concurrent access</li> <li>Document any pandas version dependencies</li> <li>Consider memory efficiency for large datasets</li> </ul>"},{"location":"architecture/overview/","title":"Architecture Overview","text":""},{"location":"architecture/overview/#causaliq-ecosystem","title":"CausalIQ Ecosystem","text":"<p>causaliq-data is a component of the overall CausalIQ ecosystem architecture, providing the data layer foundation for causal discovery algorithms.</p>"},{"location":"architecture/overview/#core-architecture-plug-in-data-adapters","title":"Core Architecture: Plug-in Data Adapters","text":"<p>CausalIQ Data is built around a plug-in data adapter architecture that enables seamless integration of different data sources and formats through a unified interface. This design provides flexibility while maintaining consistent performance characteristics across different data backends.</p>"},{"location":"architecture/overview/#abstract-base-class-data","title":"Abstract Base Class (<code>Data</code>)","text":"<p>The <code>Data</code> class defines the core BNFit interface that all data adapters must implement. This abstract base class:</p> <ul> <li>Extends the BNFit interface from causaliq-core for Bayesian Network fitting</li> <li>Defines standard methods for data access, manipulation, and randomisation</li> <li>Ensures consistent behavior across all concrete implementations</li> <li>Provides common functionality like node ordering and name randomisation</li> </ul>"},{"location":"architecture/overview/#concrete-data-adapters","title":"Concrete Data Adapters","text":"<p>The architecture supports multiple data adapters, each optimized for different use cases:</p> <ol> <li>Pandas Adapter - For standard tabular data with rich type support</li> <li>NumPy Adapter - For high-performance numerical operations on large datasets</li> <li>Oracle Adapter - For synthetic data generation from known Bayesian Networks</li> </ol>"},{"location":"architecture/overview/#key-architectural-features","title":"Key Architectural Features","text":""},{"location":"architecture/overview/#in-memory-counting-and-optimization","title":"In-Memory Counting and Optimization","text":"<p>The data adapters implement sophisticated in-memory counting mechanisms for efficient statistical operations:</p> <ul> <li>Categorical Data Counting: Optimized binning and counting for discrete variables using NumPy's <code>bincount</code> functionality</li> <li>Value Combination Caching: Intelligent caching of unique value combinations to avoid recomputation</li> <li>Memory-Efficient Storage: Strategic use of appropriate data types (int16, int32, float32, float64) to minimize memory footprint</li> <li>Sample Subset Management: Efficient handling of data subsets without copying underlying arrays</li> </ul>"},{"location":"architecture/overview/#data-randomisation-capabilities","title":"Data Randomisation Capabilities","text":"<p>The architecture provides comprehensive data randomisation features essential for causal discovery validation:</p>"},{"location":"architecture/overview/#node-name-randomisation","title":"Node Name Randomisation","text":"<ul> <li>Purpose: Assess algorithm sensitivity to variable naming</li> <li>Implementation: Systematic generation of randomized node names while preserving data relationships</li> <li>Reversibility: Ability to revert to original names for result interpretation</li> </ul>"},{"location":"architecture/overview/#sample-order-randomisation","title":"Sample Order Randomisation","text":"<ul> <li>Purpose: Test algorithm stability across different data presentations  </li> <li>Methods: Multiple randomization strategies (full shuffle, random selection, seeded ordering)</li> <li>Seed Management: Deterministic randomization for reproducible experiments</li> </ul>"},{"location":"architecture/overview/#node-processing-order-randomisation","title":"Node Processing Order Randomisation","text":"<ul> <li>Purpose: Evaluate algorithm sensitivity to variable processing order</li> <li>Flexibility: Support for custom orderings or random permutations</li> <li>Preservation: Maintains data integrity while changing algorithmic perspectives</li> </ul>"},{"location":"architecture/overview/#performance-optimizations","title":"Performance Optimizations","text":""},{"location":"architecture/overview/#lazy-evaluation","title":"Lazy Evaluation","text":"<ul> <li>Sample subsets are computed on-demand rather than pre-computed</li> <li>Type conversions happen only when necessary (e.g., float64 conversion for continuous data during scoring)</li> </ul>"},{"location":"architecture/overview/#memory-management","title":"Memory Management","text":"<ul> <li>Original data is preserved separately from working samples</li> <li>Efficient copy-on-write semantics where possible</li> <li>Strategic use of views vs copies to minimize memory overhead</li> </ul>"},{"location":"architecture/overview/#algorithmic-efficiency","title":"Algorithmic Efficiency","text":"<ul> <li>Optimized unique value detection using both numpy.unique and custom counting approaches</li> <li>Threshold-based algorithm selection for optimal performance across different data sizes</li> <li>In-place operations where safe and beneficial</li> </ul>"},{"location":"architecture/overview/#integration-points","title":"Integration Points","text":""},{"location":"architecture/overview/#causaliq-core-integration","title":"CausalIQ Core Integration","text":"<ul> <li>Implements BNFit interface for seamless integration with Bayesian Network fitting algorithms</li> <li>Provides marginal distributions and conditional independence testing capabilities</li> <li>Supports parameter estimation workflows</li> </ul>"},{"location":"architecture/overview/#causaliq-discovery-integration","title":"CausalIQ Discovery Integration","text":"<ul> <li>Supplies objective functions for score-based structure learning</li> <li>Provides conditional independence tests for constraint-based algorithms</li> <li>Enables stability testing through randomisation features</li> </ul>"},{"location":"architecture/overview/#independence-testing-framework","title":"Independence Testing Framework","text":"<p>The <code>indep</code> module provides statistical independence testing capabilities essential for constraint-based causal discovery:</p>"},{"location":"architecture/overview/#test-statistics-and-methods","title":"Test Statistics and Methods","text":"<ul> <li>Multiple Test Types: Chi-squared (X\u00b2) and Mutual Information (MI) statistical tests</li> <li>Conditional Independence: Support for testing X \u22a5 Y | Z with arbitrary conditioning variable sets</li> <li>Asymptotic Theory: Both test statistics are asymptotically \u03c7\u00b2 distributed under the null hypothesis</li> </ul>"},{"location":"architecture/overview/#data-source-flexibility","title":"Data Source Flexibility","text":"<ul> <li>DataFrame Integration: Direct testing on pandas DataFrame data</li> <li>File System Support: Automatic data loading from CSV files</li> <li>Bayesian Network Parameters: Synthetic testing using known conditional probability tables</li> <li>Unified Interface: Consistent API regardless of data source</li> </ul>"},{"location":"architecture/overview/#computational-efficiency","title":"Computational Efficiency","text":"<ul> <li>Contingency Table Optimization: Efficient construction and manipulation of multi-dimensional contingency tables</li> <li>Batch Processing: Simultaneous computation of multiple test statistics</li> <li>Memory Management: Minimal data copying during statistical computations</li> </ul>"},{"location":"architecture/overview/#score-function-architecture","title":"Score Function Architecture","text":"<p>The <code>score</code> module provides a comprehensive scoring framework for evaluating Bayesian network structures:</p>"},{"location":"architecture/overview/#multi-type-score-support","title":"Multi-Type Score Support","text":"<ul> <li>Categorical Scores: BIC, AIC, log-likelihood, BDE, K2, and other Bayesian methods</li> <li>Gaussian Scores: BGE (Bayesian Gaussian Equivalent), Gaussian BIC, and continuous log-likelihood</li> <li>Mixed Data: Automatic score type selection based on variable types</li> </ul>"},{"location":"architecture/overview/#modular-design","title":"Modular Design","text":"<ul> <li>Node-Level Scoring: Independent evaluation of individual nodes with their parents</li> <li>Network-Level Scoring: Complete DAG and Bayesian Network evaluation with per-node breakdowns</li> <li>Parameter Validation: Centralized parameter checking and default assignment</li> </ul>"},{"location":"architecture/overview/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Efficient Counting: Leverages data adapter counting mechanisms for marginal distributions</li> <li>Vectorized Operations: Uses NumPy operations for mathematical computations</li> <li>Memory Efficiency: Minimal data copying during score computation</li> </ul>"},{"location":"architecture/overview/#causaliq-workflow-integration","title":"CausalIQ Workflow Integration","text":"<ul> <li>Supports experimental workflows requiring data randomization</li> <li>Provides consistent interfaces for batch processing</li> <li>Enables reproducible research through seed management</li> </ul>"},{"location":"architecture/overview/#design-principles","title":"Design Principles","text":"<ol> <li>Separation of Concerns: Data access, transformation, and algorithm logic are clearly separated</li> <li>Performance by Design: Architecture prioritizes computational efficiency for large-scale causal discovery</li> <li>Extensibility: New data adapters can be added without changing existing code</li> <li>Type Safety: Comprehensive type checking and validation throughout the pipeline</li> <li>Reproducibility: Built-in support for seeded randomization and deterministic operations</li> </ol>"},{"location":"userguide/introduction/","title":"CausalIQ Data User Guide","text":"<p>To be completed.</p>"}]}