{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83d\udd22  CausalIQ Data","text":""},{"location":"#welcome","title":"Welcome","text":"<p>Welcome to the documentation for CausalIQ Data \u2014 part of the CausalIQ ecosystem for intelligent causal discovery. </p> <p>The CausalIQ Data project provides the data-related capabilities that causal discovery requires. </p>"},{"location":"#overview","title":"Overview","text":"<p>CausalIQ Data provides:</p> <ul> <li>\u26a1 data import and caching - data can be imported from standard  tabular formats (comma-separated variables) and cached for high performance</li> <li>\ud83c\udfaf graph scoring - provide graph score derived from the data which is  the objective function used by score-based structure learning algorithms. This is   based upon how likely the data is to be seen for a given graph, typically  modified by a penalty for complex graphs (e.g. BIC score), or modified  by a prior belief about the graph strcuture (e.g. BDeu score)</li> <li>\ud83d\udd17 independence tests - used to determine conditional independence tests  which are intrinsic to the operataion of constraint-based structure  learning algorithms.</li> </ul> <p>This site provides detailed documentation, including: development roadmap, user guide, architectural vision, design notes, and API reference for users and contributors.</p>"},{"location":"#quickstart-installation","title":"Quickstart &amp; Installation","text":"<p>For a quickstart guide and installation instructions, see the README on GitHub.</p>"},{"location":"#documentation-contents","title":"Documentation Contents","text":"<ul> <li>Development Roadmap: roadmap of upcoming features</li> <li>User Guide: comprehensive user guide</li> <li>Architecture: overall architecture and design notes</li> <li>API Reference: complete reference for Python code</li> <li>Development Guidelines: CausalIQ guidelines for developers</li> <li>Changelog</li> <li>License</li> </ul>"},{"location":"#support-community","title":"Support &amp; Community","text":"<ul> <li>GitHub Issues: Report bugs or request features.</li> <li>GitHub Discussions: Ask questions and join the community.</li> </ul> <p>Tip: Use the navigation sidebar to explore the documentation. For the latest code and releases, visit the causaliq-data GitHub repository.</p> <p>Supported Python Versions: 3.9, 3.10, 3.11, 3.12 Default Python Version: 3.11</p>"},{"location":"roadmap/","title":"CausalIQ Data - Development Roadmap","text":"<p>Last updated: December 16, 2025  </p> <p>This project roadmap fits into the overall ecosystem roadmap</p>"},{"location":"roadmap/#under-development","title":"\ud83d\udea7  Under development","text":"<ul> <li>none</li> </ul>"},{"location":"roadmap/#implemented-features","title":"\u2705 Implemented Features","text":"<ul> <li>Release v0.1.0 - Foundation Data [December 2025]: CausalIQ compliant Data provider interface and concrete implementations with data store internally as pandas Dataframes or Numpy 2D arrays.</li> </ul> <p>See Git commit history for detailed implementation progress</p>"},{"location":"roadmap/#upcoming-releases","title":"\ud83d\udee3\ufe0f Upcoming Releases","text":"<ul> <li>Release v0.2.0 - Score [December 2025]: Support for BIC and BDeu score functions</li> <li>Release v0.3.0 - CI Tests [December 2025]: Conditional Independence</li> </ul>"},{"location":"api/cli/","title":"Data CLI","text":"<p>This template repo provides a simple template CLI as a starting point for CLI implementations.</p>"},{"location":"api/cli/#cli-entry-point","title":"CLI entry point","text":"<p>This is the entry point for the CLI logic.</p>"},{"location":"api/cli/#causaliq_data.cli","title":"cli","text":"<p>Command-line interface for causaliq-data.</p> <p>Functions:</p> <ul> <li> <code>cli</code>             \u2013              <p>Simple CLI example.</p> </li> <li> <code>main</code>             \u2013              <p>Entry point for the CLI.</p> </li> </ul>"},{"location":"api/cli/#causaliq_data.cli.cli","title":"cli","text":"<pre><code>cli(name: str, greet: str) -&gt; None\n</code></pre> <p>Simple CLI example.</p> <p>NAME is the person to greet</p>"},{"location":"api/cli/#causaliq_data.cli.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>Entry point for the CLI.</p>"},{"location":"api/data/","title":"Data - Abstract Base Class","text":"<p>The <code>Data</code> class serves as the abstract base class for all data adapters in CausalIQ Data. It extends the BNFit interface from causaliq-core and provides the foundation for the plug-in data adapter architecture.</p>"},{"location":"api/data/#class-definition","title":"Class Definition","text":"<pre><code>class Data(BNFit):\n    \"\"\"Top level data object that implements BNFit interface.\n\n    Extends BNFit interface with additional methods needed for\n    causal structure learning algorithms.\n    \"\"\"\n</code></pre>"},{"location":"api/data/#key-attributes","title":"Key Attributes","text":"<ul> <li><code>elapsed</code>: Elapsed time for operations</li> <li><code>order</code>: Order in which nodes should be processed</li> <li><code>ext_to_orig</code>: Mapping from external to original node names</li> <li><code>orig_to_ext</code>: Mapping from original to external node names  </li> <li><code>dstype</code>: Overall dataset type (categorical/continuous/mixed)</li> </ul>"},{"location":"api/data/#core-methods","title":"Core Methods","text":""},{"location":"api/data/#node-order-management","title":"Node Order Management","text":""},{"location":"api/data/#set_orderorder-tuplestr-none","title":"<code>set_order(order: Tuple[str, ...]) -&gt; None</code>","text":"<p>Sets the processing order of nodes to the specified sequence.</p> <p>Arguments: - <code>order</code>: New processing order as a tuple of external node names</p> <p>Raises: - <code>TypeError</code>: For invalid argument types - <code>ValueError</code>: If order contains invalid node names</p>"},{"location":"api/data/#get_order-tuplestr","title":"<code>get_order() -&gt; Tuple[str, ...]</code>","text":"<p>Returns the current processing order using external node names.</p>"},{"location":"api/data/#randomise_orderseed-int-none","title":"<code>randomise_order(seed: int) -&gt; None</code>","text":"<p>Randomizes the processing order of nodes using the specified seed.</p> <p>Arguments: - <code>seed</code>: Randomization seed (must be non-negative)</p>"},{"location":"api/data/#name-randomisation","title":"Name Randomisation","text":""},{"location":"api/data/#randomise_namesseed-optionalint-none","title":"<code>randomise_names(seed: Optional[int]) -&gt; None</code>","text":"<p>Randomizes node names for sensitivity testing. When <code>seed=None</code>, reverts to original names.</p> <p>Arguments: - <code>seed</code>: Randomization seed or None to revert</p>"},{"location":"api/data/#abstract-methods","title":"Abstract Methods","text":"<p>The following methods must be implemented by concrete subclasses:</p>"},{"location":"api/data/#sample-management","title":"Sample Management","text":"<ul> <li><code>set_N(N, seed, random_selection)</code>: Set working sample size with optional randomization</li> <li><code>_update_sample(old_N, old_ext_to_orig)</code>: Update sample after parameter changes</li> </ul>"},{"location":"api/data/#data-access","title":"Data Access","text":"<ul> <li><code>marginals(node, parents, values_reqd)</code>: Get marginal distributions</li> <li><code>values(nodes)</code>: Return values for specified nodes  </li> <li><code>as_df()</code>: Convert to pandas DataFrame representation</li> </ul>"},{"location":"api/data/#persistence","title":"Persistence","text":"<ul> <li><code>write(filename)</code>: Write data to file</li> </ul>"},{"location":"api/data/#properties-from-bnfit","title":"Properties (from BNFit)","text":""},{"location":"api/data/#node-information","title":"Node Information","text":"<ul> <li><code>nodes</code>: Internal (original) node names</li> <li><code>node_types</code>: Node type mapping (e.g., <code>{node1: type1, ...}</code>)</li> <li><code>node_values</code>: Value counts for categorical nodes</li> </ul>"},{"location":"api/data/#sample-information","title":"Sample Information","text":"<ul> <li><code>N</code>: Current sample size being used</li> <li><code>sample</code>: Access to underlying data sample</li> </ul>"},{"location":"api/data/#type-system","title":"Type System","text":""},{"location":"api/data/#datasettype-enumeration","title":"DatasetType Enumeration","text":"<pre><code>class DatasetType(StrEnum):\n    CATEGORICAL = \"categorical\"  # All categorical variables\n    CONTINUOUS = \"continuous\"    # All float variables  \n    MIXED = \"mixed\"             # Mixed categorical and numeric\n</code></pre>"},{"location":"api/data/#variabletype-enumeration","title":"VariableType Enumeration","text":"<pre><code>class VariableType(StrEnum):\n    INT16 = \"int16\"\n    INT32 = \"int32\" \n    INT64 = \"int64\"\n    FLOAT32 = \"float32\"\n    FLOAT64 = \"float64\"\n    CATEGORY = \"category\"\n</code></pre>"},{"location":"api/data/#internal-methods","title":"Internal Methods","text":""},{"location":"api/data/#_set_dstype-none","title":"<code>_set_dstype() -&gt; None</code>","text":"<p>Determines the overall dataset type from individual node types. Sets <code>dstype</code> to:</p> <ul> <li><code>CATEGORICAL</code>: If all variables are categorical</li> <li><code>CONTINUOUS</code>: If all variables are numeric  </li> <li><code>MIXED</code>: If variables are mixed types</li> </ul>"},{"location":"api/data/#_generate_random_namesseed-optionalint-none","title":"<code>_generate_random_names(seed: Optional[int]) -&gt; None</code>","text":"<p>Generates randomized external names for nodes using the format <code>X###NNNNNN</code> where:</p> <ul> <li><code>###</code> is a zero-padded random integer</li> <li><code>NNNNNN</code> is the first 6 characters of the original name</li> </ul> <p>When <code>seed=None</code>, reverts mappings back to original names.</p>"},{"location":"api/data/#usage-patterns","title":"Usage Patterns","text":""},{"location":"api/data/#basic-inheritance","title":"Basic Inheritance","text":"<pre><code>class CustomData(Data):\n    def __init__(self, source):\n        super().__init__()\n        # Initialize from source\n\n    def set_N(self, N, seed=None, random_selection=False):\n        # Implement sample size setting\n\n    def marginals(self, node, parents, values_reqd=False):\n        # Implement marginal computation\n\n    # ... implement other abstract methods\n</code></pre>"},{"location":"api/data/#name-randomization-workflow","title":"Name Randomization Workflow","text":"<pre><code># Store original order for later restoration\noriginal_order = data.get_order()\n\n# Randomize names for sensitivity testing\ndata.randomise_names(seed=42)\n\n# Run algorithm with randomized names\nresults_randomized = run_algorithm(data)\n\n# Restore original names\ndata.randomise_names(seed=None)\n\n# Compare results\ncompare_sensitivity(results_original, results_randomized)\n</code></pre>"},{"location":"api/data/#integration-with-bnfit","title":"Integration with BNFit","text":"<p>The Data class fully implements the BNFit interface, enabling seamless integration with causaliq-core components:</p> <ul> <li>Parameter Estimation: Compatible with BN parameter fitting algorithms</li> <li>Score Calculation: Provides necessary marginals for score-based learning</li> <li>Constraint Testing: Supports conditional independence testing workflows</li> </ul>"},{"location":"api/numpy/","title":"NumPy - High-Performance Array Adapter","text":"<p>The <code>NumPy</code> class provides a high-performance implementation of the Data interface using NumPy arrays as the underlying storage. This adapter is optimized for computational efficiency and large-scale causal discovery operations.</p>"},{"location":"api/numpy/#class-definition","title":"Class Definition","text":"<pre><code>class NumPy(Data):\n    \"\"\"Concrete Data subclass which holds data in NumPy arrays.\n\n    Args:\n        data (ndarray): Data provided as a 2-D NumPy array.\n        dstype (DatasetType): Type of variables in dataset.\n        col_values (dict): Column names and their categorical values.\n\n    Attributes:\n        data (ndarray): The original data values.\n        sample (ndarray): Sample values of size N, rows possibly reordered.\n        categories: Categories for each categorical node.\n    \"\"\"\n</code></pre>"},{"location":"api/numpy/#constructor","title":"Constructor","text":""},{"location":"api/numpy/#__init__data-dstype-col_valuesnone-none","title":"<code>__init__(data, dstype, col_values=None) -&gt; None</code>","text":"<p>Creates a NumPy data adapter from a 2D NumPy array.</p> <p>Arguments: - <code>data</code>: 2D NumPy array with shape (n_samples, n_features) - <code>dstype</code>: Dataset type (<code>DatasetType.CATEGORICAL</code>, <code>CONTINUOUS</code>, or <code>MIXED</code>) - <code>col_values</code>: Optional mapping of column names to categorical values</p> <p>Validation: - Data must be 2D NumPy array - Minimum 2 samples and 2 features required - For categorical data, values must be integer-encoded starting from 0</p> <p>Initialization: - Sets up node names as X0, X1, X2, ... by default - Converts categorical values to appropriate categories - Determines node types based on dstype</p>"},{"location":"api/numpy/#factory-methods","title":"Factory Methods","text":""},{"location":"api/numpy/#from_dfdf-dstype-keep_dffalse-numpy","title":"<code>from_df(df, dstype, keep_df=False) -&gt; 'NumPy'</code>","text":"<p>Creates a NumPy instance from a pandas DataFrame.</p> <p>Arguments: - <code>df</code>: Pandas DataFrame containing the data - <code>dstype</code>: Target dataset type for conversion - <code>keep_df</code>: Whether to preserve DataFrame for <code>as_df()</code> operations</p> <p>Features: - Automatic conversion from pandas to NumPy format - Intelligent handling of categorical data encoding - Optional DataFrame preservation for round-trip compatibility</p> <p>Example: <pre><code># Convert from Pandas\npandas_data = Pandas.read(\"data.csv\", dstype=\"categorical\")\nnumpy_data = NumPy.from_df(pandas_data.as_df(), \n                          dstype=\"categorical\", \n                          keep_df=True)\n</code></pre></p>"},{"location":"api/numpy/#high-performance-operations","title":"High-Performance Operations","text":""},{"location":"api/numpy/#set_nn-seednone-random_selectionfalse-none","title":"<code>set_N(N, seed=None, random_selection=False) -&gt; None</code>","text":"<p>Sets working sample size with optimized sampling strategies.</p> <p>Arguments:</p> <ul> <li><code>N</code>: Target sample size</li> <li><code>seed</code>: Random seed for reproducible results</li> <li><code>random_selection</code>: Use random subset vs first N rows</li> </ul> <p>Performance Features:</p> <ul> <li>Random Selection: Uses <code>numpy.random.choice()</code> for efficient random sampling</li> <li>Row Shuffling: Optional in-place shuffling with <code>permutation()</code></li> <li>Memory Optimization: Works with array views when possible</li> <li>Type Conversion: Converts continuous data to float64 for precision only when needed</li> </ul> <p>Implementation Details: <pre><code># Efficient random selection without replacement\nindices = rng.choice(self.data.shape[0], size=N, replace=False)\nself._sample = self.data[indices if seed != 0 else sorted(indices)]\n\n# In-place row order randomization\nif seed is not None and seed != 0:\n    order = rng.permutation(N)\n    self._sample = self.sample[order]\n</code></pre></p>"},{"location":"api/numpy/#uniquej_reqd-num_vals-tuplendarray-ndarray","title":"<code>unique(j_reqd, num_vals) -&gt; Tuple[ndarray, ndarray]</code>","text":"<p>Highly optimized unique value detection and counting.</p> <p>Arguments:</p> <ul> <li><code>j_reqd</code>: Tuple of column indices for which unique combinations are needed</li> <li><code>num_vals</code>: Array of number of unique values per column</li> </ul> <p>Returns:</p> <ul> <li><code>(combinations, counts)</code>: Unique value combinations and their frequencies</li> </ul> <p>Optimization Strategy: <pre><code># Fast path for small combination spaces\nmax_combinations = prod(num_vals)\nif max_combinations &lt;= THRESHOLD:\n    # Use integer packing for ultra-fast counting\n    # Pack multiple values into single integers\n    multipliers = [prod(num_vals[i+1:]) for i in range(len(num_vals))]\n    packed = dot(self.sample[:, j_reqd], multipliers)\n    counts = bincount(packed)\n    # Unpack results efficiently\nelse:\n    # Fall back to numpy.unique for large spaces\n    combos, counts = npunique(self.sample[:, j_reqd], \n                             axis=0, return_counts=True)\n</code></pre></p>"},{"location":"api/numpy/#in-memory-counting-optimizations","title":"In-Memory Counting Optimizations","text":""},{"location":"api/numpy/#categorical-value-counting","title":"Categorical Value Counting","text":"<pre><code># Ultra-fast categorical counting using bincount\nfor j in range(self.sample.shape[1]):\n    counts = {\n        self.categories[j][v]: c \n        for v, c in enumerate(bincount(self.sample[:, j]))\n    }\n    self._node_values[node_name] = {v: counts[v] for v in sorted(counts)}\n</code></pre>"},{"location":"api/numpy/#memory-efficient-storage","title":"Memory-Efficient Storage","text":"<ul> <li>Uses minimal integer types for categorical data (typically int16 or int32)</li> <li>Lazy conversion to float64 only for continuous scoring operations</li> <li>Strategic copying vs view usage to minimize memory footprint</li> </ul>"},{"location":"api/numpy/#advanced-sampling","title":"Advanced Sampling","text":""},{"location":"api/numpy/#random-selection-strategies","title":"Random Selection Strategies","text":"<p>Random Subset Selection: <pre><code>data.set_N(1000, seed=42, random_selection=True)\n# Randomly selects 1000 rows from dataset\n</code></pre></p> <p>Ordered Sampling with Shuffling: <pre><code>data.set_N(1000, seed=42, random_selection=False)\n# Uses first 1000 rows but randomizes their order\n</code></pre></p>"},{"location":"api/numpy/#deterministic-reproducibility","title":"Deterministic Reproducibility","text":"<ul> <li>Seed=0 and seed=None both preserve original data order</li> <li>Positive seeds enable reproducible randomization</li> <li>Consistent behavior across multiple calls with same seed</li> </ul>"},{"location":"api/numpy/#statistical-operations","title":"Statistical Operations","text":""},{"location":"api/numpy/#marginalsnode-parents-values_reqdfalse-tuple","title":"<code>marginals(node, parents, values_reqd=False) -&gt; Tuple</code>","text":"<p>Efficient marginal computation using NumPy operations.</p> <p>Implementation:</p> <ul> <li>Leverages optimized <code>unique()</code> method for counting</li> <li>Handles sparse parent configurations efficiently</li> <li>Returns results in format compatible with scoring algorithms</li> </ul>"},{"location":"api/numpy/#valuesnodes-ndarray","title":"<code>values(nodes) -&gt; ndarray</code>","text":"<p>Direct array access for specified columns.</p> <p>Performance:</p> <ul> <li>Returns views when possible to avoid copying</li> <li>Maintains column order as specified</li> <li>Efficient slicing for subset access</li> </ul>"},{"location":"api/numpy/#memory-management","title":"Memory Management","text":""},{"location":"api/numpy/#data-storage-strategy","title":"Data Storage Strategy","text":"<pre><code>self.data        # Original immutable data\nself._sample     # Current working sample (possibly reordered)\nself.categories  # Categorical value mappings (shared across samples)\n</code></pre>"},{"location":"api/numpy/#copy-on-write-semantics","title":"Copy-on-Write Semantics","text":"<ul> <li>Original data never modified</li> <li>Sample arrays created as views when order unchanged</li> <li>Copies created only when shuffling or subset selection required</li> </ul>"},{"location":"api/numpy/#type-optimization","title":"Type Optimization","text":"<ul> <li>Categorical data stored as smallest possible integer type</li> <li>Continuous data uses float32 by default, converted to float64 only for scoring</li> <li>Automatic type inference minimizes memory usage</li> </ul>"},{"location":"api/numpy/#name-randomization","title":"Name Randomization","text":""},{"location":"api/numpy/#randomise_namesseednone-none","title":"<code>randomise_names(seed=None) -&gt; None</code>","text":"<p>Efficient node name randomization without data copying.</p> <p>Features:</p> <ul> <li>Updates only mapping dictionaries, not underlying arrays</li> <li>Preserves all data relationships and types</li> <li>Updates cached node_values and node_types dictionaries consistently</li> </ul>"},{"location":"api/numpy/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"api/numpy/#typical-performance-characteristics","title":"Typical Performance Characteristics","text":"<p>Memory Usage:</p> <ul> <li>~50-80% less memory than equivalent pandas DataFrame</li> <li>Categorical data: ~2-4 bytes per value vs 8+ bytes in pandas</li> <li>Continuous data: 4 bytes (float32) vs 8 bytes (float64) by default</li> </ul> <p>Computational Speed:</p> <ul> <li>Unique value detection: 10-100x faster than pandas for categorical data</li> <li>Sample subset creation: 5-20x faster than DataFrame operations</li> <li>Marginal calculations: 20-50x faster for large datasets</li> </ul> <p>Scalability:</p> <ul> <li>Efficiently handles datasets with millions of rows</li> <li>Linear scaling with data size for most operations</li> <li>Memory usage scales predictably with dataset dimensions</li> </ul>"},{"location":"api/numpy/#integration-examples","title":"Integration Examples","text":""},{"location":"api/numpy/#high-performance-workflow","title":"High-Performance Workflow","text":"<pre><code># Load and convert for performance\npandas_data = Pandas.read(\"large_dataset.csv\", dstype=\"categorical\")\nnumpy_data = NumPy.from_df(pandas_data.as_df(), \n                          dstype=\"categorical\")\n\n# Set large working sample efficiently\nnumpy_data.set_N(100000, seed=42, random_selection=True)\n\n# Perform intensive causal discovery\nresults = heavy_computation_algorithm(numpy_data)\n</code></pre>"},{"location":"api/numpy/#memory-conscious-processing","title":"Memory-Conscious Processing","text":"<pre><code># Process data in chunks for memory efficiency\nfor chunk_seed in range(10):\n    numpy_data.set_N(10000, seed=chunk_seed, random_selection=True)\n    chunk_results = process_chunk(numpy_data)\n    aggregate_results(chunk_results)\n</code></pre>"},{"location":"api/numpy/#benchmarking-and-experimentation","title":"Benchmarking and Experimentation","text":"<pre><code># Performance comparison across randomizations\ntiming_results = []\nfor trial in range(100):\n    numpy_data.randomise_order(seed=trial)\n    start_time = time.time()\n    result = algorithm.run(numpy_data)\n    timing_results.append(time.time() - start_time)\n\nprint(f\"Mean runtime: {np.mean(timing_results):.3f}s\")\nprint(f\"Std deviation: {np.std(timing_results):.3f}s\")\n</code></pre>"},{"location":"api/numpy/#best-practices","title":"Best Practices","text":""},{"location":"api/numpy/#when-to-use-numpy-adapter","title":"When to Use NumPy Adapter","text":"<ul> <li>Large datasets (&gt;10K rows typically)</li> <li>Performance-critical causal discovery algorithms</li> <li>Memory-constrained environments</li> <li>Repeated statistical computations</li> <li>Benchmark and stability experiments requiring many randomizations</li> </ul>"},{"location":"api/numpy/#optimization-tips","title":"Optimization Tips","text":"<ul> <li>Use <code>random_selection=True</code> only when needed (creates copy)</li> <li>Convert from Pandas early in pipeline for consistent performance</li> <li>Leverage <code>keep_df=True</code> only if round-trip DataFrame access needed</li> <li>Choose appropriate <code>dstype</code> for your data characteristics</li> </ul>"},{"location":"api/oracle/","title":"Oracle - Synthetic Data Generator","text":"<p>The <code>Oracle</code> class provides a specialized data adapter that generates synthetic data from known Bayesian Networks. This adapter is primarily used for algorithm validation, benchmarking, and controlled experiments where the true underlying causal structure is known.</p>"},{"location":"api/oracle/#class-definition","title":"Class Definition","text":"<pre><code>class Oracle(Data):\n    \"\"\"Oracle data adapter for synthetic data generation from Bayesian Networks.\n\n    Args:\n        bn: A BN (Bayesian Network) object from causaliq-core.\n\n    Attributes:\n        bn: The underlying Bayesian Network object.\n    \"\"\"\n</code></pre>"},{"location":"api/oracle/#constructor","title":"Constructor","text":""},{"location":"api/oracle/#__init__bn-none","title":"<code>__init__(bn) -&gt; None</code>","text":"<p>Creates an Oracle data adapter from a Bayesian Network.</p> <p>Arguments: - <code>bn</code>: BN object from causaliq-core containing DAG structure and CPTs</p> <p>Validation: - Input must be a valid BN object - BN must contain both DAG structure and conditional probability tables - All nodes must have associated conditional distributions</p> <p>Initialization: - Extracts node names from BN DAG structure - Determines variable types from conditional distributions (CPT vs continuous) - Sets initial sample size to 1 (can be changed with <code>set_N()</code>)</p> <p>Example: <pre><code>from causaliq_core.bn.io import read_bn\nfrom causaliq_data import Oracle\n\n# Load BN from file\nbn = read_bn(\"cancer.dsc\")\n\n# Create Oracle adapter\noracle = Oracle(bn)\nprint(f\"Nodes: {oracle.nodes}\")\nprint(f\"Types: {oracle.node_types}\")\n</code></pre></p>"},{"location":"api/oracle/#synthetic-data-generation","title":"Synthetic Data Generation","text":""},{"location":"api/oracle/#set_nn-seednone-random_selectionfalse-none","title":"<code>set_N(N, seed=None, random_selection=False) -&gt; None</code>","text":"<p>Sets the effective sample size for synthetic data operations.</p> <p>Arguments:</p> <ul> <li><code>N</code>: Target sample size for synthetic data generation</li> <li><code>seed</code>: Must be None (not supported for Oracle)</li> <li><code>random_selection</code>: Must be False (not applicable)</li> </ul> <p>Behavior:</p> <ul> <li>Updates internal sample size counter</li> <li>Does not actually generate data (Oracle provides analytical answers)</li> <li>Used by algorithms to determine confidence/precision of estimates</li> </ul> <p>Validation:</p> <ul> <li><code>N</code> must be positive integer</li> <li><code>seed</code> parameter must be None (raises TypeError if provided)</li> <li><code>random_selection</code> must be False</li> </ul> <p>Usage: <pre><code>oracle.set_N(10000)  # Set effective sample size\nprint(f\"Sample size: {oracle.N}\")\n</code></pre></p>"},{"location":"api/oracle/#statistical-operations","title":"Statistical Operations","text":""},{"location":"api/oracle/#marginalsnode-parents-values_reqdfalse-tuple","title":"<code>marginals(node, parents, values_reqd=False) -&gt; Tuple</code>","text":"<p>Provides exact marginal distributions from the Bayesian Network.</p> <p>Arguments:</p> <ul> <li><code>node</code>: Target node name (internal name)</li> <li><code>parents</code>: Dictionary specifying parent values <code>{parent: value}</code></li> <li><code>values_reqd</code>: Whether to return value labels (always False for Oracle)</li> </ul> <p>Returns:</p> <ul> <li>Exact conditional probability distribution for the node given parents</li> <li>For categorical nodes: probability vector over possible values</li> <li>For continuous nodes: parameters of the conditional distribution</li> </ul> <p>Features:</p> <ul> <li>Exact Results: Returns true probabilities, not empirical estimates</li> <li>No Sampling Error: Results are analytical, not subject to sampling variation</li> <li>Efficient Computation: Leverages BN's internal probability representations</li> </ul> <p>Example: <pre><code># Get P(Cancer | Smoker=True, Pollution=High)\nmarginal = oracle.marginals(\"Cancer\", \n                           {\"Smoker\": \"True\", \"Pollution\": \"High\"})\nprint(f\"P(Cancer=True|evidence): {marginal[0][1]}\")\n</code></pre></p>"},{"location":"api/oracle/#valuesnodes-npndarray","title":"<code>values(nodes) -&gt; np.ndarray</code>","text":"<p>Not Implemented: Oracle does not store actual data values.</p> <p>Raises:</p> <ul> <li><code>TypeError</code>: Always raised with message \"Oracle.values() not implemented\"</li> </ul> <p>Rationale:</p> <ul> <li>Oracle provides analytical results, not sampled data</li> <li>Use concrete adapters (Pandas/NumPy) for data value access</li> <li>Consistent with Oracle's role as synthetic probability source</li> </ul>"},{"location":"api/oracle/#specialized-oracle-features","title":"Specialized Oracle Features","text":""},{"location":"api/oracle/#true-parameter-access","title":"True Parameter Access","text":"<p>Oracle provides direct access to the true parameters of the Bayesian Network:</p> <p>Conditional Probability Tables: <pre><code># Access true CPT for a categorical node\ncpt = oracle.bn.cnds[\"Disease\"]\nprint(\"True conditional probabilities:\")\nfor parent_config in cpt.parents_configs():\n    for value in cpt.values:\n        prob = cpt.get_prob(parent_config, value)\n        print(f\"P({value}|{parent_config}) = {prob}\")\n</code></pre></p> <p>Network Structure: <pre><code># Access true DAG structure\ndag = oracle.bn.dag\nprint(f\"True edges: {dag.edges}\")\nprint(f\"True parents of X: {dag.parents('X')}\")\n</code></pre></p>"},{"location":"api/oracle/#algorithm-validation","title":"Algorithm Validation","text":"<p>Oracle is ideal for validating causal discovery algorithms:</p> <p>Score Validation: <pre><code># Compare algorithm scores with true model\ntrue_score = oracle.score(learned_dag)\noracle_score = oracle.score(oracle.bn.dag)  # True structure score\nprint(f\"Score difference: {abs(true_score - oracle_score)}\")\n</code></pre></p> <p>Conditional Independence Testing: <pre><code># Test algorithm's CI conclusions against true model\nfor x, y, z in ci_tests:\n    true_independent = oracle.bn.d_separated(x, y, z)\n    algorithm_independent = algorithm.ci_test(oracle, x, y, z)\n    accuracy = (true_independent == algorithm_independent)\n</code></pre></p>"},{"location":"api/oracle/#limitations-and-constraints","title":"Limitations and Constraints","text":""},{"location":"api/oracle/#unsupported-operations","title":"Unsupported Operations","text":"<p>Data Value Access: - <code>values()</code> method raises TypeError - No actual data samples available - Use for probability queries only</p> <p>Randomization Restrictions:</p> <ul> <li><code>randomise_names()</code> raises NotImplementedError  </li> <li>Name randomization not meaningful for Oracle</li> <li>Node names tied to BN structure</li> </ul> <p>Sampling Limitations:</p> <ul> <li>No row-level sampling or shuffling</li> <li><code>set_N()</code> only affects effective sample size for algorithms</li> <li>No actual data generation performed</li> </ul>"},{"location":"api/oracle/#data-type-constraints","title":"Data Type Constraints","text":"<p>Variable Types:</p> <ul> <li>Categorical variables: Must have finite discrete values</li> <li>Continuous variables: Limited to supported distribution types</li> <li>Mixed networks: Handled according to individual node types</li> </ul>"},{"location":"api/oracle/#integration-with-causaliq-ecosystem","title":"Integration with CausalIQ Ecosystem","text":""},{"location":"api/oracle/#algorithm-testing-framework","title":"Algorithm Testing Framework","text":"<pre><code>def test_algorithm_accuracy(algorithm, test_bns):\n    results = []\n    for bn_file in test_bns:\n        # Load true BN\n        bn = read_bn(bn_file)\n        oracle = Oracle(bn)\n\n        # Run algorithm\n        oracle.set_N(10000)  # Large effective sample size\n        learned_structure = algorithm.run(oracle)\n\n        # Compare with true structure  \n        accuracy = compare_structures(bn.dag, learned_structure)\n        results.append(accuracy)\n\n    return results\n</code></pre>"},{"location":"api/oracle/#benchmark-experiments","title":"Benchmark Experiments","text":"<pre><code>def benchmark_scoring_functions(oracle, scoring_functions):\n    true_score = {}\n    for score_fn in scoring_functions:\n        # Get score for true structure\n        true_score[score_fn.name] = score_fn.calculate(oracle, oracle.bn.dag)\n\n        # Test alternative structures\n        for alt_structure in generate_alternatives(oracle.bn.dag):\n            alt_score = score_fn.calculate(oracle, alt_structure)\n            print(f\"{score_fn.name}: True={true_score[score_fn.name]:.3f}, \"\n                  f\"Alt={alt_score:.3f}\")\n</code></pre>"},{"location":"api/oracle/#stability-analysis","title":"Stability Analysis","text":"<pre><code>def analyze_algorithm_stability(algorithm, oracle, trials=100):\n    # Oracle provides consistent \"data\" across trials\n    results = []\n    for trial in range(trials):\n        oracle.randomise_order(trial)  # Change processing order\n        result = algorithm.run(oracle)\n        results.append(result)\n\n    # Analyze consistency of results\n    return assess_stability(results)\n</code></pre>"},{"location":"api/oracle/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"api/oracle/#computational-efficiency","title":"Computational Efficiency","text":"<ul> <li>Analytical Operations: No sampling or counting required</li> <li>Exact Computations: Probability queries return exact values</li> <li>Memory Efficient: No large data arrays stored</li> <li>Fast Initialization: Only stores BN structure and parameters</li> </ul>"},{"location":"api/oracle/#scalability","title":"Scalability","text":"<ul> <li>Network Size: Performance depends on BN complexity, not sample size</li> <li>Query Complexity: Marginal queries scale with network connectivity</li> <li>Memory Usage: Minimal, proportional to BN size only</li> </ul>"},{"location":"api/oracle/#best-practices","title":"Best Practices","text":""},{"location":"api/oracle/#when-to-use-oracle","title":"When to Use Oracle","text":"<ul> <li>Algorithm Validation: Testing against known ground truth</li> <li>Benchmarking: Comparing algorithm performance across known structures</li> <li>Method Development: Developing new algorithms with reliable test cases</li> <li>Educational Use: Demonstrating causal discovery concepts</li> </ul>"},{"location":"api/oracle/#usage-patterns","title":"Usage Patterns","text":"<pre><code># Validation workflow\noracle = Oracle(known_bn)\noracle.set_N(sample_size)\n\n# Test your algorithm\nlearned_result = your_algorithm.discover(oracle)\n\n# Compare with truth\naccuracy_metrics = evaluate_against_truth(learned_result, oracle.bn)\n</code></pre>"},{"location":"api/oracle/#integration-tips","title":"Integration Tips","text":"<ul> <li>Use Oracle early in algorithm development for debugging</li> <li>Combine with Pandas/NumPy adapters for comprehensive testing</li> <li>Leverage exact probabilities for theoretical analysis</li> <li>Document true structure properties for result interpretation</li> </ul>"},{"location":"api/overview/","title":"CausalIQ Data API Reference","text":"<p>The CausalIQ Data API provides a unified interface for data handling in causal discovery workflows. The API is built around a plug-in architecture with concrete implementations for different data backends.</p>"},{"location":"api/overview/#core-design","title":"Core Design","text":"<p>All data adapters implement the <code>Data</code> abstract base class, which extends the BNFit interface from causaliq-core. This ensures consistent behavior across different data sources while allowing backend-specific optimizations.</p>"},{"location":"api/overview/#module-structure","title":"Module Structure","text":""},{"location":"api/overview/#data-abstract-base-class","title":"<code>Data</code> - Abstract Base Class","text":"<p>The foundational abstract class that defines the core interface for all data adapters. Provides:</p> <ul> <li>Node Management: Consistent handling of variable names and ordering</li> <li>Randomisation Framework: Built-in support for data and name randomisation </li> <li>BNFit Interface: Full compatibility with Bayesian Network fitting operations</li> <li>Type System: Unified variable type handling across data sources</li> </ul>"},{"location":"api/overview/#pandas-dataframe-based-adapter","title":"<code>Pandas</code> - DataFrame-Based Adapter","text":"<p>A concrete implementation that wraps pandas DataFrames for flexible data handling:</p> <ul> <li>Rich Type Support: Native pandas categorical and numeric types</li> <li>File I/O: Direct CSV reading with compression support</li> <li>Data Validation: Comprehensive missing data and type checking</li> <li>Memory Efficiency: Smart sampling and subsetting without data duplication</li> </ul>"},{"location":"api/overview/#numpy-high-performance-array-adapter","title":"<code>NumPy</code> - High-Performance Array Adapter","text":"<p>A high-performance implementation using NumPy arrays for computational efficiency:</p> <ul> <li>Optimized Counting: Fast categorical data counting using <code>bincount</code></li> <li>Memory Management: Efficient handling of large datasets with minimal copying</li> <li>Type Optimization: Automatic selection of appropriate numeric types</li> <li>Advanced Sampling: Multiple strategies for data subset selection and randomization</li> </ul>"},{"location":"api/overview/#oracle-synthetic-data-generator","title":"<code>Oracle</code> - Synthetic Data Generator","text":"<p>A specialized adapter for generating synthetic data from known Bayesian Networks:</p> <ul> <li>BN Integration: Direct integration with causaliq-core BN objects</li> <li>Parameter Access: Direct access to true conditional probability tables</li> <li>Testing Support: Ideal for algorithm validation and benchmarking</li> <li>Simulation Control: Flexible sample size management for experiments</li> </ul>"},{"location":"api/overview/#common-patterns","title":"Common Patterns","text":""},{"location":"api/overview/#data-loading","title":"Data Loading","text":"<pre><code>from causaliq_data import Pandas, NumPy\n\n# Load from CSV file\ndata = Pandas.read(\"dataset.csv\", dstype=\"categorical\")\n\n# Convert to NumPy for performance\nnumpy_data = NumPy.from_df(data.as_df(), dstype=\"categorical\")\n</code></pre>"},{"location":"api/overview/#randomisation-workflows","title":"Randomisation Workflows","text":"<pre><code># Randomize node names for sensitivity testing\ndata.randomise_names(seed=42)\n\n# Randomize processing order\ndata.randomise_order(seed=123)\n\n# Set working sample size with randomization\ndata.set_N(1000, seed=456, random_selection=True)\n</code></pre>"},{"location":"api/overview/#statistical-operations","title":"Statistical Operations","text":"<pre><code># Get marginal distributions\nmarginals = data.marginals(\"target_node\", {\"parent1\": 0, \"parent2\": 1})\n\n# Access value counts for categorical variables\ncounts = data.node_values[\"categorical_var\"]\n\n# Get unique value combinations\nunique_vals, counts = data.unique((\"var1\", \"var2\"), num_vals)\n</code></pre>"},{"location":"api/overview/#type-system","title":"Type System","text":"<p>The API supports a comprehensive type system for different variable types:</p> <ul> <li>Categorical: <code>VariableType.CATEGORY</code> for discrete variables</li> <li>Integers: <code>INT16</code>, <code>INT32</code>, <code>INT64</code> for integer data</li> <li>Floats: <code>FLOAT32</code>, <code>FLOAT64</code> for continuous variables</li> </ul> <p>Dataset types are automatically inferred:</p> <ul> <li>Categorical: All variables are categorical</li> <li>Continuous: All variables are numeric</li> <li>Mixed: Combination of categorical and numeric variables</li> </ul>"},{"location":"api/overview/#performance-considerations","title":"Performance Considerations","text":""},{"location":"api/overview/#memory-efficiency","title":"Memory Efficiency","text":"<ul> <li>Original data is preserved separately from working samples</li> <li>Lazy evaluation of expensive operations</li> <li>Strategic use of data views vs copies</li> </ul>"},{"location":"api/overview/#computational-optimization","title":"Computational Optimization","text":"<ul> <li>NumPy adapter provides the best performance for large datasets</li> <li>Optimized algorithms for unique value detection and counting</li> <li>Efficient handling of categorical data through integer encoding</li> </ul>"},{"location":"api/overview/#scalability","title":"Scalability","text":"<ul> <li>Support for working with data subsets without loading entire datasets</li> <li>Memory-conscious type selection based on data characteristics</li> <li>Configurable thresholds for algorithm selection</li> </ul>"},{"location":"api/overview/#error-handling","title":"Error Handling","text":"<p>All adapters provide comprehensive error handling with descriptive messages:</p> <ul> <li>Type Validation: Strict checking of argument types and values</li> <li>Data Validation: Detection of missing data, invalid formats, and size constraints</li> <li>State Consistency: Validation of internal state consistency across operations</li> </ul>"},{"location":"api/pandas/","title":"Pandas - DataFrame-Based Data Adapter","text":"<p>The <code>Pandas</code> class provides a concrete implementation of the Data interface using pandas DataFrames as the underlying data storage. This adapter is ideal for exploratory data analysis and moderate-sized datasets where pandas' rich functionality is beneficial.</p>"},{"location":"api/pandas/#class-definition","title":"Class Definition","text":"<pre><code>class Pandas(Data):\n    \"\"\"Data subclass which holds data in a Pandas dataframe.\n\n    Args:\n        df: Data provided as a Pandas dataframe.\n\n    Attributes:\n        df: Original Pandas dataframe providing data.\n        dstype: Type of dataset (categorical/numeric/mixed).\n    \"\"\"\n</code></pre>"},{"location":"api/pandas/#constructor","title":"Constructor","text":""},{"location":"api/pandas/#__init__df-dataframe-none","title":"<code>__init__(df: DataFrame) -&gt; None</code>","text":"<p>Creates a new Pandas data adapter from a DataFrame.</p> <p>Arguments:</p> <ul> <li><code>df</code>: Pandas DataFrame containing the data</li> </ul> <p>Validation:</p> <ul> <li>Minimum 2 rows and 2 columns required</li> <li>No missing data (NaN values) allowed</li> <li>All column names must be strings</li> </ul> <p>Raises:</p> <ul> <li><code>TypeError</code>: If df is not a pandas DataFrame</li> <li><code>ValueError</code>: If DataFrame size or data validation fails</li> </ul>"},{"location":"api/pandas/#class-methods","title":"Class Methods","text":""},{"location":"api/pandas/#readfilename-dstype-nnone-kwargs-pandas","title":"<code>read(filename, dstype, N=None, **kwargs) -&gt; 'Pandas'</code>","text":"<p>Factory method to create a Pandas instance by reading data from a file.</p> <p>Arguments:</p> <ul> <li><code>filename</code>: Path to data file (supports .csv, .gz compression)</li> <li><code>dstype</code>: Dataset type (\"categorical\", \"continuous\", or \"mixed\")  </li> <li><code>N</code>: Optional sample size limit</li> <li><code>**kwargs</code>: Additional arguments for pandas.read_csv()</li> </ul> <p>Features: - Automatic compression detection for .gz files - Intelligent type inference and conversion - Categorical variable encoding - Memory-efficient loading for large files</p> <p>Example: <pre><code># Load categorical data\ndata = Pandas.read(\"dataset.csv\", dstype=\"categorical\")\n\n# Load with custom separator and sample size\ndata = Pandas.read(\"data.tsv\", dstype=\"mixed\", N=10000, sep='\\t')\n</code></pre></p>"},{"location":"api/pandas/#data-management","title":"Data Management","text":""},{"location":"api/pandas/#set_nn-seednone-random_selectionfalse-none","title":"<code>set_N(N, seed=None, random_selection=False) -&gt; None</code>","text":"<p>Sets the working sample size with optional randomization.</p> <p>Arguments:</p> <ul> <li><code>N</code>: Target sample size (must be \u2264 original data size)</li> <li><code>seed</code>: Randomization seed for reproducible sampling</li> <li><code>random_selection</code>: If True, randomly selects rows; if False, uses first N rows</li> </ul> <p>Behavior:</p> <ul> <li>Updates internal <code>_sample</code> DataFrame with the specified subset</li> <li>Preserves original data in <code>df</code> attribute</li> <li>Recomputes categorical value counts for the new sample</li> </ul>"},{"location":"api/pandas/#randomise_namesseednone-none","title":"<code>randomise_names(seed=None) -&gt; None</code>","text":"<p>Randomizes node names for algorithm sensitivity testing.</p> <p>Arguments:</p> <ul> <li><code>seed</code>: Randomization seed, or None to revert to original names</li> </ul> <p>Implementation:</p> <ul> <li>Generates randomized column names using format <code>X###NNNNNN</code></li> <li>Updates DataFrame column names in-place</li> <li>Maintains mappings between original and external names</li> <li>Updates sample DataFrame to reflect name changes</li> </ul>"},{"location":"api/pandas/#statistical-operations","title":"Statistical Operations","text":""},{"location":"api/pandas/#marginalsnode-parents-values_reqdfalse-tuple","title":"<code>marginals(node, parents, values_reqd=False) -&gt; Tuple</code>","text":"<p>Computes marginal distributions for a node given its parents.</p> <p>Arguments:</p> <ul> <li><code>node</code>: Target node name (external name)</li> <li><code>parents</code>: Dictionary of parent values <code>{parent_name: value}</code></li> <li><code>values_reqd</code>: If True, returns actual values; if False, returns counts only</li> </ul> <p>Returns:</p> <ul> <li>Tuple of (marginal_counts, unique_values) for categorical data</li> <li>For continuous data, returns appropriate statistical summaries</li> </ul> <p>Implementation:</p> <ul> <li>Uses pandas crosstab for efficient categorical marginalization</li> <li>Handles continuous variables with binning strategies</li> <li>Optimized for sparse parent configurations</li> </ul>"},{"location":"api/pandas/#valuesnodes-tuplestr-npndarray","title":"<code>values(nodes: Tuple[str, ...]) -&gt; np.ndarray</code>","text":"<p>Returns the actual data values for specified nodes.</p> <p>Arguments:</p> <ul> <li><code>nodes</code>: Tuple of node names (external names)</li> </ul> <p>Returns:</p> <ul> <li>NumPy array with shape (N, len(nodes)) containing the data values</li> </ul> <p>Usage: <pre><code># Get values for specific variables\nsubset = data.values((\"var1\", \"var2\", \"var3\"))\nprint(subset.shape)  # (N, 3)\n</code></pre></p>"},{"location":"api/pandas/#properties","title":"Properties","text":""},{"location":"api/pandas/#node-information","title":"Node Information","text":"<ul> <li><code>nodes</code>: Original column names from DataFrame</li> <li><code>node_types</code>: Mapping of node names to their data types</li> <li><code>node_values</code>: Value counts for categorical variables only</li> </ul>"},{"location":"api/pandas/#sample-access","title":"Sample Access","text":"<ul> <li><code>N</code>: Current working sample size</li> <li><code>sample</code>: Current working sample as DataFrame</li> </ul>"},{"location":"api/pandas/#data-conversion","title":"Data Conversion","text":""},{"location":"api/pandas/#as_df-dataframe","title":"<code>as_df() -&gt; DataFrame</code>","text":"<p>Returns the current working sample as a pandas DataFrame.</p> <p>Returns:</p> <ul> <li>DataFrame with external column names and current sample data</li> </ul> <p>Usage: <pre><code># Access current sample\ncurrent_df = data.as_df()\n\n# Convert to NumPy for performance-critical operations\nnumpy_data = NumPy.from_df(current_df, dstype=data.dstype)\n</code></pre></p>"},{"location":"api/pandas/#file-io","title":"File I/O","text":""},{"location":"api/pandas/#writefilename-compressfalse-sf10-zeronone-preservetrue-none","title":"<code>write(filename, compress=False, sf=10, zero=None, preserve=True) -&gt; None</code>","text":"<p>Writes the current sample to a CSV file.</p> <p>Arguments:</p> <ul> <li><code>filename</code>: Output file path</li> <li><code>compress</code>: Whether to gzip compress the output</li> <li><code>sf</code>: Significant figures for floating-point data</li> <li><code>zero</code>: Value to replace zeros with (for numerical stability)</li> <li><code>preserve</code>: Whether to preserve original formatting</li> </ul> <p>Features:</p> <ul> <li>Automatic compression if filename ends with .gz</li> <li>Configurable precision for floating-point output</li> <li>Handles categorical data appropriately</li> <li>Preserves data integrity during round-trip operations</li> </ul>"},{"location":"api/pandas/#type-handling","title":"Type Handling","text":""},{"location":"api/pandas/#automatic-type-inference","title":"Automatic Type Inference","text":"<p>The Pandas adapter automatically infers and converts data types:</p> <p>Categorical Data:</p> <ul> <li>String columns are converted to pandas categorical type</li> <li>Integer columns with limited unique values become categorical</li> <li>Maintains category ordering where applicable</li> </ul> <p>Numeric Data:</p> <ul> <li>Floating-point columns preserve precision</li> <li>Integer columns use appropriate NumPy integer types</li> <li>Mixed columns are handled according to dstype parameter</li> </ul>"},{"location":"api/pandas/#type-validation","title":"Type Validation","text":"<pre><code># Dataset type is automatically determined\nif data.dstype == \"categorical\":\n    print(\"All variables are categorical\")\nelif data.dstype == \"continuous\":\n    print(\"All variables are numeric\")  \nelse:  # \"mixed\"\n    print(\"Mixed variable types detected\")\n</code></pre>"},{"location":"api/pandas/#memory-management","title":"Memory Management","text":""},{"location":"api/pandas/#efficient-sampling","title":"Efficient Sampling","text":"<ul> <li>Original DataFrame is preserved in <code>df</code> attribute</li> <li>Working sample stored separately in <code>_sample</code></li> <li>View-based operations where possible to minimize copying</li> </ul>"},{"location":"api/pandas/#lazy-operations","title":"Lazy Operations","text":"<ul> <li>Type conversions performed only when necessary</li> <li>Value counts computed on-demand for categorical variables</li> <li>Sample updates triggered only when needed</li> </ul>"},{"location":"api/pandas/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"api/pandas/#best-use-cases","title":"Best Use Cases","text":"<ul> <li>Exploratory data analysis and prototyping</li> <li>Moderate-sized datasets (&lt; 100K rows typically)</li> <li>Mixed data types requiring pandas functionality</li> <li>File I/O with various formats and options</li> </ul>"},{"location":"api/pandas/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Memory overhead due to pandas metadata</li> <li>String operations can be slow for large categorical data</li> <li>DataFrame operations generally slower than pure NumPy</li> </ul>"},{"location":"api/pandas/#integration-examples","title":"Integration Examples","text":""},{"location":"api/pandas/#with-numpy-adapter","title":"With NumPy Adapter","text":"<pre><code># Load and explore with Pandas\npandas_data = Pandas.read(\"data.csv\", dstype=\"mixed\")\nprint(pandas_data.as_df().describe())\n\n# Convert to NumPy for computational efficiency\nnumpy_data = NumPy.from_df(pandas_data.as_df(), \n                          dstype=pandas_data.dstype,\n                          keep_df=True)\n</code></pre>"},{"location":"api/pandas/#workflow-integration","title":"Workflow Integration","text":"<pre><code># Load data\ndata = Pandas.read(\"experiment.csv\", dstype=\"categorical\")\n\n# Set up experiment parameters\ndata.set_N(5000, seed=42, random_selection=True)\ndata.randomise_names(seed=123)\n\n# Run causal discovery algorithm\nresults = discovery_algorithm.run(data)\n\n# Save results\ndata.write(\"experiment_sample.csv\", compress=True)\n</code></pre>"},{"location":"architecture/bnfit_interface_spec/","title":"CausalIQ Data Interface Specification","text":""},{"location":"architecture/bnfit_interface_spec/#overview","title":"Overview","text":"<p>This document defines the interface contract between CausalIQ Core and CausalIQ Data packages. The core package contains BN fitting algorithms (CPT.fit, LinGauss.fit) that require data access operations. The data package will provide concrete implementations of data sources.</p>"},{"location":"architecture/bnfit_interface_spec/#abstract-data-interface","title":"Abstract Data Interface","text":""},{"location":"architecture/bnfit_interface_spec/#bnfit-abstract-base-class","title":"<code>BNFit</code> (Abstract Base Class)","text":"<p>The core package requires data sources to implement this interface:</p> <pre><code>from abc import ABC, abstractmethod\nfrom typing import Dict, Tuple, Any\nimport numpy as np\n\nclass BNFit(ABC):\n    \"\"\"Abstract interface for data sources used in BN fitting.\"\"\"\n\n    @abstractmethod\n    def marginals(self, node: str, parents: Dict, values_reqd: bool = False) -&gt; Tuple:\n        \"\"\"Return marginal counts for a node and its parents.\n\n        Args:\n            node (str): Node for which marginals required.\n            parents (dict): {node: parents} parents of non-orphan nodes\n            values_reqd (bool): Whether parent and child values required\n\n        Returns:\n            tuple: Of counts, and optionally, values:\n                   - ndarray counts: 2D, rows=child, cols=parents\n                   - int maxcol: Maximum number of parental values\n                   - tuple rowval: Child values for each row\n                   - tuple colval: Parent combo (dict) for each col\n\n        Raises:\n            TypeError: For bad argument types\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def values(self, columns: Tuple[str, ...]) -&gt; np.ndarray:\n        \"\"\"Return the (float) values for the specified set of columns.\n\n        Suitable for passing into e.g. linearRegression fitting function\n\n        Args:\n            columns (tuple): Columns for which data required\n\n        Returns:\n            ndarray: Numpy array of values, each column for a node\n\n        Raises:\n            TypeError: If bad arg type\n            ValueError: If bad arg value\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def N(self) -&gt; int:\n        \"\"\"Total sample size.\n\n        Returns:\n            int: Current sample size being used\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def node_values(self) -&gt; Dict[str, Dict]:\n        \"\"\"Node value counts for categorical variables.\n\n        Returns:\n            dict: Values and their counts of categorical nodes\n                  in sample {n1: {v1: c1, v2: ...}, n2 ...}\n        \"\"\"\n        pass\n</code></pre>"},{"location":"architecture/bnfit_interface_spec/#usage-in-core-package","title":"Usage in Core Package","text":""},{"location":"architecture/bnfit_interface_spec/#cptfit-dependencies","title":"CPT.fit() Dependencies","text":"<p>The CPT.fit() method requires these data operations:</p> <pre><code># For nodes with parents\ncounts, _, rowval, colval = data.marginals(node, {node: list(parents)}, True)\n\n# For autocomplete functionality\ndata.N  # Total sample size\ndata.node_values[node]  # {value: count} for node\ndata.node_values[parent]  # {value: count} for each parent\n\n# For orphan nodes\ndata.N\ndata.node_values[node]\n</code></pre>"},{"location":"architecture/bnfit_interface_spec/#lingaussfit-dependencies","title":"LinGauss.fit() Dependencies","text":"<p>The LinGauss.fit() method requires:</p> <pre><code># Get continuous values for regression\nvalues = data.values((node,))  # For orphan nodes\nvalues = data.values(tuple([node] + list(parents)))  # For nodes with parents\n</code></pre>"},{"location":"architecture/bnfit_interface_spec/#expected-concrete-implementations","title":"Expected Concrete Implementations","text":"<p>The data package should provide these concrete classes:</p>"},{"location":"architecture/bnfit_interface_spec/#1-legacypandasadapter","title":"1. <code>LegacyPandasAdapter</code>","text":"<ul> <li>Adapts existing legacy.data.Pandas class to DataInterface</li> <li>Delegates to existing marginals(), values(), N, node_values implementations</li> <li>Handles pandas DataFrames efficiently with crosstab-based marginals</li> <li>Ensures backward compatibility with existing test suites</li> </ul>"},{"location":"architecture/bnfit_interface_spec/#2-legacynumpyadapter","title":"2. <code>LegacyNumPyAdapter</code>","text":"<ul> <li>Adapts existing legacy.data.NumPy class to BNFit interface  </li> <li>Delegates to existing marginals(), values(), N, node_values implementations</li> <li>Enables NumPy support that doesn't currently work with core algorithms</li> <li>For NumPy-based data sources</li> </ul>"},{"location":"architecture/bnfit_interface_spec/#future-extensions","title":"Future Extensions","text":"<p>The interface is designed to be extensible for: - GPU-accelerated data sources (e.g., CuPy, Rapids cuDF) - Database backends (SQL, NoSQL) - Streaming data sources  - Distributed data processing (Dask, Spark) - Custom data transformations</p>"},{"location":"architecture/bnfit_interface_spec/#legacy-compatibility-requirements","title":"Legacy Compatibility Requirements","text":"<p>The data package must maintain compatibility with existing usage patterns:</p> <pre><code># Existing legacy pattern that must continue working\nfrom legacy.data.pandas import Pandas\ndata = Pandas(df)\ncnd_spec, estimated = CPT.fit('B', ('A',), data)\n\n# New pattern with adapter\nfrom causaliq_data import LegacyDataAdapter\ndata_adapted = LegacyDataAdapter(data)\ncnd_spec, estimated = CPT.fit('B', ('A',), data_adapted)\n</code></pre>"},{"location":"architecture/bnfit_interface_spec/#key-implementation-details","title":"Key Implementation Details","text":""},{"location":"architecture/bnfit_interface_spec/#marginals-method-behavior","title":"marginals() Method Behavior","text":"<p>For orphan nodes (no parents): - <code>parents</code> parameter: <code>{}</code> or <code>{node: []}</code> - Returns: <code>(counts.reshape(-1, 1), 1, rowval, colval)</code> - <code>rowval</code>: tuple of node values - <code>colval</code>: tuple containing single empty dict <code>({},)</code></p> <p>For nodes with single parent: - <code>parents</code> parameter: <code>{node: [parent_name]}</code> - Returns: <code>(counts_2d, num_cols, rowval, colval)</code> - <code>rowval</code>: tuple of child values - <code>colval</code>: tuple of dicts <code>({parent: value},)</code></p> <p>For nodes with multiple parents: - <code>parents</code> parameter: <code>{node: [parent1, parent2, ...]}</code> - Returns: <code>(counts_2d, num_cols, rowval, colval)</code> - <code>colval</code>: tuple of dicts <code>({parent1: val1, parent2: val2},)</code></p>"},{"location":"architecture/bnfit_interface_spec/#values-method-behavior","title":"values() Method Behavior","text":"<ul> <li>Must return numpy array with float dtype</li> <li>Each column corresponds to a requested node</li> <li>Row order must be consistent with data source</li> <li>Should validate that all requested columns exist</li> </ul>"},{"location":"architecture/bnfit_interface_spec/#error-handling-standards","title":"Error Handling Standards","text":"<p>All methods should raise: - <code>TypeError</code>: For incorrect argument types - <code>ValueError</code>: For invalid argument values (missing columns, etc.)</p>"},{"location":"architecture/bnfit_interface_spec/#migration-path","title":"Migration Path","text":"<ol> <li>Phase 1: Create data package with interface and implementations</li> <li>Phase 2: Update core package to import DataInterface from data package</li> <li>Phase 3: Update legacy tests to use adapters</li> <li>Phase 4: Add new data source types as needed</li> </ol>"},{"location":"architecture/bnfit_interface_spec/#testing-requirements","title":"Testing Requirements","text":"<p>The data package should include: - Unit tests for each concrete implementation - Integration tests with core CPT.fit() and LinGauss.fit() - Compatibility tests with legacy test suite - Performance benchmarks for marginals calculation</p>"},{"location":"architecture/bnfit_interface_spec/#future-extensions_1","title":"Future Extensions","text":"<p>The interface is designed to be extensible for: - Database backends - Streaming data sources - Distributed data processing - Custom data transformations</p>"},{"location":"architecture/bnfit_interface_spec/#notes-for-implementation","title":"Notes for Implementation","text":"<ul> <li>Prioritize performance in marginals() calculation (this is the bottleneck)</li> <li>Consider caching computed marginals for repeated queries</li> <li>Ensure thread safety if needed for concurrent access</li> <li>Document any pandas version dependencies</li> <li>Consider memory efficiency for large datasets</li> </ul>"},{"location":"architecture/overview/","title":"Architecture Overview","text":""},{"location":"architecture/overview/#causaliq-ecosystem","title":"CausalIQ Ecosystem","text":"<p>causaliq-data is a component of the overall CausalIQ ecosystem architecture, providing the data layer foundation for causal discovery algorithms.</p>"},{"location":"architecture/overview/#core-architecture-plug-in-data-adapters","title":"Core Architecture: Plug-in Data Adapters","text":"<p>CausalIQ Data is built around a plug-in data adapter architecture that enables seamless integration of different data sources and formats through a unified interface. This design provides flexibility while maintaining consistent performance characteristics across different data backends.</p>"},{"location":"architecture/overview/#abstract-base-class-data","title":"Abstract Base Class (<code>Data</code>)","text":"<p>The <code>Data</code> class defines the core BNFit interface that all data adapters must implement. This abstract base class:</p> <ul> <li>Extends the BNFit interface from causaliq-core for Bayesian Network fitting</li> <li>Defines standard methods for data access, manipulation, and randomisation</li> <li>Ensures consistent behavior across all concrete implementations</li> <li>Provides common functionality like node ordering and name randomisation</li> </ul>"},{"location":"architecture/overview/#concrete-data-adapters","title":"Concrete Data Adapters","text":"<p>The architecture supports multiple data adapters, each optimized for different use cases:</p> <ol> <li>Pandas Adapter - For standard tabular data with rich type support</li> <li>NumPy Adapter - For high-performance numerical operations on large datasets</li> <li>Oracle Adapter - For synthetic data generation from known Bayesian Networks</li> </ol>"},{"location":"architecture/overview/#key-architectural-features","title":"Key Architectural Features","text":""},{"location":"architecture/overview/#in-memory-counting-and-optimization","title":"In-Memory Counting and Optimization","text":"<p>The data adapters implement sophisticated in-memory counting mechanisms for efficient statistical operations:</p> <ul> <li>Categorical Data Counting: Optimized binning and counting for discrete variables using NumPy's <code>bincount</code> functionality</li> <li>Value Combination Caching: Intelligent caching of unique value combinations to avoid recomputation</li> <li>Memory-Efficient Storage: Strategic use of appropriate data types (int16, int32, float32, float64) to minimize memory footprint</li> <li>Sample Subset Management: Efficient handling of data subsets without copying underlying arrays</li> </ul>"},{"location":"architecture/overview/#data-randomisation-capabilities","title":"Data Randomisation Capabilities","text":"<p>The architecture provides comprehensive data randomisation features essential for causal discovery validation:</p>"},{"location":"architecture/overview/#node-name-randomisation","title":"Node Name Randomisation","text":"<ul> <li>Purpose: Assess algorithm sensitivity to variable naming</li> <li>Implementation: Systematic generation of randomized node names while preserving data relationships</li> <li>Reversibility: Ability to revert to original names for result interpretation</li> </ul>"},{"location":"architecture/overview/#sample-order-randomisation","title":"Sample Order Randomisation","text":"<ul> <li>Purpose: Test algorithm stability across different data presentations  </li> <li>Methods: Multiple randomization strategies (full shuffle, random selection, seeded ordering)</li> <li>Seed Management: Deterministic randomization for reproducible experiments</li> </ul>"},{"location":"architecture/overview/#node-processing-order-randomisation","title":"Node Processing Order Randomisation","text":"<ul> <li>Purpose: Evaluate algorithm sensitivity to variable processing order</li> <li>Flexibility: Support for custom orderings or random permutations</li> <li>Preservation: Maintains data integrity while changing algorithmic perspectives</li> </ul>"},{"location":"architecture/overview/#performance-optimizations","title":"Performance Optimizations","text":""},{"location":"architecture/overview/#lazy-evaluation","title":"Lazy Evaluation","text":"<ul> <li>Sample subsets are computed on-demand rather than pre-computed</li> <li>Type conversions happen only when necessary (e.g., float64 conversion for continuous data during scoring)</li> </ul>"},{"location":"architecture/overview/#memory-management","title":"Memory Management","text":"<ul> <li>Original data is preserved separately from working samples</li> <li>Efficient copy-on-write semantics where possible</li> <li>Strategic use of views vs copies to minimize memory overhead</li> </ul>"},{"location":"architecture/overview/#algorithmic-efficiency","title":"Algorithmic Efficiency","text":"<ul> <li>Optimized unique value detection using both numpy.unique and custom counting approaches</li> <li>Threshold-based algorithm selection for optimal performance across different data sizes</li> <li>In-place operations where safe and beneficial</li> </ul>"},{"location":"architecture/overview/#integration-points","title":"Integration Points","text":""},{"location":"architecture/overview/#causaliq-core-integration","title":"CausalIQ Core Integration","text":"<ul> <li>Implements BNFit interface for seamless integration with Bayesian Network fitting algorithms</li> <li>Provides marginal distributions and conditional independence testing capabilities</li> <li>Supports parameter estimation workflows</li> </ul>"},{"location":"architecture/overview/#causaliq-discovery-integration","title":"CausalIQ Discovery Integration","text":"<ul> <li>Supplies objective functions for score-based structure learning</li> <li>Provides conditional independence tests for constraint-based algorithms</li> <li>Enables stability testing through randomisation features</li> </ul>"},{"location":"architecture/overview/#causaliq-workflow-integration","title":"CausalIQ Workflow Integration","text":"<ul> <li>Supports experimental workflows requiring data randomization</li> <li>Provides consistent interfaces for batch processing</li> <li>Enables reproducible research through seed management</li> </ul>"},{"location":"architecture/overview/#design-principles","title":"Design Principles","text":"<ol> <li>Separation of Concerns: Data access, transformation, and algorithm logic are clearly separated</li> <li>Performance by Design: Architecture prioritizes computational efficiency for large-scale causal discovery</li> <li>Extensibility: New data adapters can be added without changing existing code</li> <li>Type Safety: Comprehensive type checking and validation throughout the pipeline</li> <li>Reproducibility: Built-in support for seeded randomization and deterministic operations</li> </ol>"},{"location":"userguide/introduction/","title":"CausalIQ Data User Guide","text":"<p>To be completed.</p>"}]}